{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPU_ImageAug_Final.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L-cV4f5FiOK",
        "colab_type": "code",
        "outputId": "fbcbe183-dafc-4e42-b60f-ac49034b8816",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nih-JDbgFzRw",
        "colab_type": "code",
        "outputId": "a3a283f3-9037-49ad-c5ed-03324504c4d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-08 02:18:08--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.68.10\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248100043 (237M) [application/zip]\n",
            "Saving to: ‘tiny-imagenet-200.zip’\n",
            "\n",
            "tiny-imagenet-200.z 100%[===================>] 236.61M  65.0MB/s    in 4.7s    \n",
            "\n",
            "2019-04-08 02:18:13 (49.9 MB/s) - ‘tiny-imagenet-200.zip’ saved [248100043/248100043]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ubnyrf8-FzGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv tiny-imagenet-200.zip tinyimagenet.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gR-UZ2YvF5Gt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -qq 'tinyimagenet.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V07j8jDrGFCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv tiny-imagenet-200 tinyimagenet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjbYFJJbHOQW",
        "colab_type": "code",
        "outputId": "e44a61ba-50b6-45d5-e31f-deb0b9985f9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Activation, add\n",
        "from tensorflow.keras.layers import Conv2D, SeparableConv2D, MaxPooling2D, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.activations import relu\n",
        "from tensorflow.keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Lambda\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from keras import backend as k\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
        "\n",
        "from __future__ import print_function\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import plot_model\n",
        "\n",
        "import imgaug as ia\n",
        "from imgaug import augmenters as iaa\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import os\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 64, 64\n",
        "# The tinyimagenet images are RGB.\n",
        "img_channels = 3\n",
        "\n",
        "batch_size = 2048\n",
        "num_classes = 200\n",
        "num_epoch = 50\n",
        "data_augmentation = True\n",
        "\n",
        "path='./tinyimagenet'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIITJJz5HWV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.1, patience=10, verbose=1, mode='auto', min_delta=0.001, cooldown=0, min_lr=1e-7)\n",
        "csv_logger = CSVLogger('/content/gdrive/My Drive/TPUCustomImageAugFinalv2.csv')\n",
        "filepath=\"/content/gdrive/My Drive/epochs_TPUCustomImageAugFinalv2:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUkUrDLtHgzc",
        "colab_type": "code",
        "outputId": "634d1227-f4b7-4ef9-f94a-1f2aac673a5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "num_outputs = 200\n",
        "input = Input(shape=(64, 64, 3,))\n",
        "\n",
        "conv1 = Conv2D(64, (3,3), strides=(1,1), padding='same', kernel_regularizer=l2(1.e-4), name='conv_1')(input)\t\t\t\t\t\t\t\t\t\t#32,32,64\t\t\t3\n",
        "bn1 = BatchNormalization(name='norm_1')(conv1)\n",
        "act1 = Activation('relu')(bn1)\n",
        "conv2 = Conv2D(64, (3,3), strides=(1,1), padding='same', kernel_regularizer=l2(1.e-4), name='conv_2')(act1) \t\t\t\t\t\t\t\t\t\t#32,32,64\t\t\t5\n",
        "\n",
        "#layer1\n",
        "bn2 = BatchNormalization(name='norm_2')(conv2)\n",
        "act2 = Activation('relu')(bn2)\n",
        "conv3 = Conv2D(64, (3,3), strides=(1,1), padding='same', kernel_regularizer=l2(1.e-4), name='conv_3')(act1) \t\t\t\t\t\t\t\t\t\t#32,32,64\t\t\t7\t\t\t\t\t\t\t\t\n",
        "#add\n",
        "add1 = add([conv2, conv3]) \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t                                                                                #32,32,64\n",
        "\n",
        "bn3 = BatchNormalization(name='norm_3')(add1)\n",
        "act3 = Activation('relu')(bn3)\n",
        "conv4 = Conv2D(128, (3,3), strides=(1,1), padding='same', kernel_regularizer=l2(1.e-4), name='conv_4')(act3)\t\t\t\t\t\t\t\t#32,32,128\t\t9\t\t\t\t\t\t\t\t\n",
        "bn4 = BatchNormalization(name='norm_4')(conv4)  \n",
        "act4 = Activation('relu')(bn4)\n",
        "pool1 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"same\")(act4)\t\t\t\t\t\t\t\t\t\t\t                                          #16,16,128\t\t\t18\n",
        "\n",
        "#add\n",
        "shortconv1 = Conv2D(128, (3,3), strides=(1,1), padding='same', kernel_regularizer=l2(0.0001), name='shortconv_1')(add1)\t\t\t\t#32,32,128\t\t\t20\n",
        "shortconv11 = Conv2D(128, (1,1), strides=(2,2), padding='valid', kernel_regularizer=l2(0.0001), name='shortconv_11')(shortconv1) #16,16,128\t\t\t20\n",
        "add2 = add([shortconv11, pool1])\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t                                                                  #16,16,128\n",
        "\n",
        "#layer2\n",
        "bn5 = BatchNormalization(name='norm_5')(add2)\n",
        "act5 = Activation('relu')(bn5)\n",
        "conv5 = Conv2D(256, (3,3), strides=(1,1), padding='same', kernel_regularizer=l2(1.e-4), name='conv_5')(act5)\t\t\t\t\t\t\t\t#16,16,256\t    24 *+4\n",
        "bn6 = BatchNormalization(name='norm_6')(conv5)\n",
        "act6 = Activation('relu')(bn6)\n",
        "\n",
        "shortconv2 = Conv2D(256, (3,3), strides=(1,1), padding='same', kernel_regularizer=l2(0.0001), name='shortconv_2')(add2)\t\t\t\t#16,16,256\t\t  28\n",
        "#add\n",
        "add3 = add([shortconv2, act6])\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t                                                                  #16,16,256\n",
        "\n",
        "bn7 = BatchNormalization(name='norm_7')(add3)\n",
        "act7 = Activation('relu')(bn7)\n",
        "conv6 = SeparableConv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu', depthwise_initializer='glorot_uniform', pointwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=l2(1.e-4), pointwise_regularizer=None, bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, pointwise_constraint=None, bias_constraint=None)(act7)\n",
        "bn8 = BatchNormalization(name='norm_8')(conv6)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t                                                          #16,16,512\t\t\t32\n",
        "act8 = Activation('relu')(bn8)\n",
        "conv7 = Conv2D(512, (3,3), strides=(2,2), padding='same', kernel_regularizer=l2(1.e-4), name='conv_7')(act8)\t\t\t\t\t\t\t\t\t\t#8,8,512\t\t\t  36\n",
        "#add\n",
        "shortconv3 = Conv2D(512, (3,3), strides=(1,1), padding='same', kernel_regularizer=l2(0.0001), name='shortconv_3')(add3)\t      #8,8,512\t\t\t  44 *+8\n",
        "shortconv33 = Conv2D(512, (1,1), strides=(2,2), padding='valid', kernel_regularizer=l2(0.0001), name='shortconv_33')(shortconv3) #8,8,512\t\t\t44\n",
        "add4 = add([shortconv33, conv7])\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t                                                                  #8,8,512\n",
        "\n",
        "\n",
        "bn9 = BatchNormalization(name='norm_9')(add4)\n",
        "act9 = Activation('relu')(bn9)\n",
        "conv8 = SeparableConv2D(1024, (3, 3), strides=(1, 1), padding='same', activation='relu', depthwise_initializer='glorot_uniform', pointwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=l2(1.e-4), pointwise_regularizer=None, bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, pointwise_constraint=None, bias_constraint=None)(act9)\n",
        "bn10 = BatchNormalization(name='norm_10')(conv8)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t                                                        #8,8,1024\t\t\t  60 *+16\n",
        "act10 = Activation('relu')(bn10)\n",
        "conv9 = Conv2D(1024, (3,3), strides=(2,2), padding='same', kernel_regularizer=l2(1.e-4), name='conv_9')(act10)\t\t\t\t\t\t\t\t\t#4,4,1024\t\t\t  76\n",
        "#add\n",
        "shortconv4 = Conv2D(1024, (3,3), strides=(1,1), padding='same', kernel_regularizer=l2(0.0001), name='shortconv_4')(add4)\t\t\t#4,4,1024\t\t\t  100 *+24\n",
        "shortconv44 = Conv2D(1024, (1,1), strides=(2,2), padding='valid', kernel_regularizer=l2(0.0001), name='shortconv_44')(shortconv4) #4,4,1024\t\t\t100\n",
        "add5 = add([shortconv44, conv9])\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t                                                                  #4,4,1024\n",
        "\n",
        "bn11 = BatchNormalization(name='norm_11')(add5)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t                                                          #4,4,1024\t\t\t  164  \n",
        "act11 = Activation('relu')(bn11)\n",
        "\n",
        "\n",
        "conv15 = Conv2D(num_outputs, (1,1), strides=(1,1), name='conv_15', use_bias=False)(act11)\n",
        "pool2 = GlobalAveragePooling2D()(conv15)\n",
        "output = Activation('softmax')(pool2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaK4V89cPPAW",
        "colab_type": "code",
        "outputId": "2b9a8467-824f-48c3-db2d-3ab5dea533d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "\n",
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.61.220.130:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 8367284064408530391)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 10854564726786272880)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 5444045647809446164)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 6340760155172129931)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 14158243375386939758)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 3597960483063460635)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 10549791999535905097)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 14825175447531954655)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 11872790257155450522)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 748490085288293559)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 7428096906477454219)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZLGgOq2uYyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.losses.categorical_crossentropy\n",
        "\n",
        "tpu_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=train_loss,\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2fOZSADuZsv",
        "colab_type": "code",
        "outputId": "c8925396-cc52-4559-b5fc-4a4b07dc7ca0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1785
        }
      },
      "source": [
        "tpu_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv_1 (Conv2D)                 (None, 64, 64, 64)   1792        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "norm_1 (BatchNormalizationV1)   (None, 64, 64, 64)   256         conv_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 64, 64, 64)   0           norm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv_2 (Conv2D)                 (None, 64, 64, 64)   36928       activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv_3 (Conv2D)                 (None, 64, 64, 64)   36928       activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 64, 64, 64)   0           conv_2[0][0]                     \n",
            "                                                                 conv_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "norm_3 (BatchNormalizationV1)   (None, 64, 64, 64)   256         add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 64, 64, 64)   0           norm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv_4 (Conv2D)                 (None, 64, 64, 128)  73856       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "norm_4 (BatchNormalizationV1)   (None, 64, 64, 128)  512         conv_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "shortconv_1 (Conv2D)            (None, 64, 64, 128)  73856       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 64, 64, 128)  0           norm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "shortconv_11 (Conv2D)           (None, 32, 32, 128)  16512       shortconv_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 32, 32, 128)  0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 128)  0           shortconv_11[0][0]               \n",
            "                                                                 max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "norm_5 (BatchNormalizationV1)   (None, 32, 32, 128)  512         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 128)  0           norm_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv_5 (Conv2D)                 (None, 32, 32, 256)  295168      activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "norm_6 (BatchNormalizationV1)   (None, 32, 32, 256)  1024        conv_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "shortconv_2 (Conv2D)            (None, 32, 32, 256)  295168      add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 256)  0           norm_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 256)  0           shortconv_2[0][0]                \n",
            "                                                                 activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "norm_7 (BatchNormalizationV1)   (None, 32, 32, 256)  1024        add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 256)  0           norm_7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d (SeparableConv (None, 32, 32, 512)  133888      activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "norm_8 (BatchNormalizationV1)   (None, 32, 32, 512)  2048        separable_conv2d[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "shortconv_3 (Conv2D)            (None, 32, 32, 512)  1180160     add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 512)  0           norm_8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "shortconv_33 (Conv2D)           (None, 16, 16, 512)  262656      shortconv_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv_7 (Conv2D)                 (None, 16, 16, 512)  2359808     activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 16, 16, 512)  0           shortconv_33[0][0]               \n",
            "                                                                 conv_7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "norm_9 (BatchNormalizationV1)   (None, 16, 16, 512)  2048        add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 16, 512)  0           norm_9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_1 (SeparableCo (None, 16, 16, 1024) 529920      activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "norm_10 (BatchNormalizationV1)  (None, 16, 16, 1024) 4096        separable_conv2d_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "shortconv_4 (Conv2D)            (None, 16, 16, 1024) 4719616     add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 1024) 0           norm_10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "shortconv_44 (Conv2D)           (None, 8, 8, 1024)   1049600     shortconv_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv_9 (Conv2D)                 (None, 8, 8, 1024)   9438208     activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 8, 8, 1024)   0           shortconv_44[0][0]               \n",
            "                                                                 conv_9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "norm_11 (BatchNormalizationV1)  (None, 8, 8, 1024)   4096        add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 8, 8, 1024)   0           norm_11[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_15 (Conv2D)                (None, 8, 8, 200)    204800      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 200)          0           conv_15[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 200)          0           global_average_pooling2d[0][0]   \n",
            "==================================================================================================\n",
            "Total params: 20,724,736\n",
            "Trainable params: 20,716,800\n",
            "Non-trainable params: 7,936\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWJuQDkpv_XC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq = iaa.Sequential(\n",
        "      [\n",
        "\t\t  iaa.Sometimes(0.3, iaa.CoarseDropout((0.3), size_percent=(0.01))), #removing pixels in form of rectangles\n",
        "      iaa.Sometimes(0.3, iaa.Crop(percent=(0, 0.3))), #cropping by 30%\n",
        "\t\t  iaa.Sometimes(0.3, iaa.Affine(scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)})), #scale images to 80-120%, translate by -20 to 20% per axis\n",
        "      iaa.Sometimes(0.3, iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5))), #sharpening the images using lightness value, high lightness value makes the image brighter\n",
        "      iaa.Sometimes(0.3, iaa.OneOf([\n",
        "                    iaa.EdgeDetect(alpha=(0, 0.7)), #running edge detection kernel\n",
        "                    iaa.DirectedEdgeDetect(alpha=(0, 0.7), direction=(0.0, 1.0)),\n",
        "                    ])),\n",
        "      iaa.Sometimes(0.3, iaa.AdditiveGaussianNoise(scale=(0.1))), #adding noise to images\n",
        "      iaa.Sometimes(0.1, iaa.GaussianBlur(sigma=(0, 0.5))) #making the images blurred\n",
        "\t  ],\n",
        "\n",
        "      # do all of the above augmentations in random order\n",
        "      random_order=True\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH1WlR9x7Z2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    if epoch > 100:\n",
        "        lr = 0.5e-5\n",
        "    elif epoch > 75:\n",
        "        lr = 1e-5\n",
        "    elif epoch > 50:\n",
        "        lr = 0.5e-4\n",
        "    elif epoch > 30:\n",
        "        lr = 1e-4\n",
        "    else:\n",
        "        lr = 1e-3\n",
        "    print('Learning rate (from LearningRateScheduler): ', lr)\n",
        "    return lr\n",
        "\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_7ffui9wIT9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale= 1./255,\n",
        "    preprocessing_function = seq.augment_image\n",
        "    )\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_cMpXP4wLdC",
        "colab_type": "code",
        "outputId": "0159ddb1-5a88-4a12-b46d-b973f11fc659",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory( r'./tinyimagenet/train/', target_size=(64, 64), color_mode='rgb', \n",
        "                                                    batch_size=batch_size, class_mode='categorical')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1EWxx0AwOcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_data = pd.read_csv('./tinyimagenet/val/val_annotations.txt', sep='\\t', header=None, names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n",
        "val_data.drop(['X', 'Y', 'H', 'W'], axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcA7yxw5wUJC",
        "colab_type": "code",
        "outputId": "1001b9d8-985a-47fb-cdfa-a54e99294dcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "validation_generator = valid_datagen.flow_from_dataframe(val_data, directory='./tinyimagenet/val/images/', x_col='File', y_col='Class', target_size=(64, 64),\n",
        "                                                    color_mode='rgb', class_mode='categorical', batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10000 images belonging to 200 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDyalkQgwlUt",
        "colab_type": "code",
        "outputId": "61ccfa1d-f3f2-4ea4-e347-35229881e17e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 18686
        }
      },
      "source": [
        "tpu_model.fit_generator(train_generator, \n",
        "                        epochs=125, \n",
        "                        steps_per_epoch=int(100000//batch_size),\n",
        "                        validation_steps=int(10000//batch_size), \n",
        "                        validation_data=validation_generator,\n",
        "                        callbacks=[checkpoint, lr_reducer, csv_logger, lr_scheduler] )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 1/125\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(212,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(212, 64, 64, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(212, 200), dtype=tf.float32, name='activation_11_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py:302: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f4556fd6668> []\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 26.906020879745483 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "INFO:tensorflow:CPU -> TPU lr: 0.0010000000474974513 {0.001}\n",
            "INFO:tensorflow:CPU -> TPU beta_1: 0.8999999761581421 {0.9}\n",
            "INFO:tensorflow:CPU -> TPU beta_2: 0.9990000128746033 {0.999}\n",
            "INFO:tensorflow:CPU -> TPU decay: 0.0 {0.0}\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            " 1/49 [..............................] - ETA: 1:03:30 - loss: 5.9240 - acc: 0.0053INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(256,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(256, 64, 64, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(256, 200), dtype=tf.float32, name='activation_11_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f4556fd6668> [<tf.Variable 'tpu_139935820394736/Adam/iterations:0' shape=() dtype=int64>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f45519c0320>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4556f4fdd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4551959048>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4551973320>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4556f114e0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4556f34550>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4556e1e400>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f45517da358>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f45517faeb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4551765b38>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f455169afd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f455169abe0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f45516bd080>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f45516bdc18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f45515ebe10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f45515b0748>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f455151de80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f45514ab978>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f45514abe80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f455148b7f0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f45513dac18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f45513daba8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f455139d9e8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f455136a710>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4551334f98>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f45512c2390>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4551225f60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4551225f28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f45511d0208>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f45511bab00>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f45510c6b00>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4551090c18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f45510afcc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4551074c88>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f455103eeb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550f4fb38>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550f71e10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550f35550>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550e47f60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550e0ae10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550ea0278>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550e68358>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550d81f28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550d81e80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550c92dd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550c566a0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550c1fcf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550b89160>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550be8978>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550adbe10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550b3bcf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550a48080>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550a6dc88>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550a314a8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f455099acf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f455097f710>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f45508c8b70>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f45508f0518>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f455088d710>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f455087aa58>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f455078af60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f455078a9e8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f455071c6d8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f45506d95f8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f455073f898>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f45506a3f60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550637f28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f45505d7e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550563dd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550530898>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550497cc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550497a58>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550423f28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f45503e9b00>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f455047beb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f45503536d8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f45502e2630>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f455028bf28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f45502ad518>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550273b00>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f45501e1f60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f45501a85f8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550147748>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f455009dd30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550113a90>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f4550009470>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f455006ad68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454ff96550>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454ff96ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454ff21d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454fe8d828>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454fe54f60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454feb3940>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454fdddb38>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454fdbfef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454fdbf5c0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454fe01470>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454fc9dc50>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454fc616d8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454fcc13c8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454fbf1d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454fb58a90>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454fb7dbe0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454fb43898>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454fa759e8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454fa15d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f9dcda0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454fa00b00>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f90deb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f8d4ba8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f8bdf60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f862940>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f804d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f7f1390>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f75ba20>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f6c6780>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f6aff28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f652630>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f68ab00>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f5dbc50>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f586a20>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f510240>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f4d7d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f4ff160>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f4c1b38>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f409f28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f3f5588>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f3ba978>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f3807f0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f28fcf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f255f60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f21def0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f1e7a20>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f14cb38>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f14c6d8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f0ddf28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f0ffc88>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454f00db00>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454efd4da0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454ef9acf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454ef648d0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454ef04c50>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454ee91ba8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454ee5bb70>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454ee26630>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454ede84a8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454ed50160>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454ed76780>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454ece25f8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454eca9fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454ec6d2e8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454ec374e0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454ebfbf28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454eb69eb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454ead1fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454ea60a20>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454eababe0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454e9caf60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454e98ad68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454e955b38>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454e978da0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454e8e8fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454e8ae5c0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454e7dfb38>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f454e7ffb38>]\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 26.293901681900024 secs\n",
            "48/49 [============================>.] - ETA: 5s - loss: 5.4771 - acc: 0.0272 INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(256,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(256, 64, 64, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(256, 200), dtype=tf.float32, name='activation_11_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f4542afd5f8> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 15.262105464935303 secs\n",
            "4/5 [=======================>......] - ETA: 6s - loss: 5.8238 - acc: 0.0100 INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(226,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(226, 64, 64, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(226, 200), dtype=tf.float32, name='activation_11_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f4542afd5f8> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 11.817124128341675 secs\n",
            "5/5 [==============================] - 40s 8s/step - loss: 5.8304 - acc: 0.0095\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.00950, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:001-val_acc:0.009.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 364s 7s/step - loss: 5.4722 - acc: 0.0273 - val_loss: 5.8304 - val_acc: 0.0095\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 2/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 6.1256 - acc: 0.0051\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.00950\n",
            "49/49 [==============================] - 148s 3s/step - loss: 5.1687 - acc: 0.0419 - val_loss: 6.1256 - val_acc: 0.0051\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 3/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 6.0196 - acc: 0.0046\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.00950\n",
            "49/49 [==============================] - 179s 4s/step - loss: 5.0060 - acc: 0.0523 - val_loss: 6.0196 - val_acc: 0.0046\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 4/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 5.7763 - acc: 0.0052\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.00950\n",
            "49/49 [==============================] - 178s 4s/step - loss: 4.8146 - acc: 0.0692 - val_loss: 5.7763 - val_acc: 0.0052\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 5/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 5.5932 - acc: 0.0073\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.00950\n",
            "49/49 [==============================] - 179s 4s/step - loss: 4.5579 - acc: 0.0948 - val_loss: 5.5932 - val_acc: 0.0073\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 6/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 5.5581 - acc: 0.0131\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.00950 to 0.01310, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:006-val_acc:0.013.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 206s 4s/step - loss: 4.5422 - acc: 0.0983 - val_loss: 5.5581 - val_acc: 0.0131\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 7/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 5.4346 - acc: 0.0205\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.01310 to 0.02050, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:007-val_acc:0.020.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 181s 4s/step - loss: 4.3726 - acc: 0.1168 - val_loss: 5.4346 - val_acc: 0.0205\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 8/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 5.3213 - acc: 0.0172\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.02050\n",
            "49/49 [==============================] - 154s 3s/step - loss: 4.2023 - acc: 0.1395 - val_loss: 5.3213 - val_acc: 0.0172\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 9/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 5.5197 - acc: 0.0086\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.02050\n",
            "49/49 [==============================] - 180s 4s/step - loss: 4.0353 - acc: 0.1645 - val_loss: 5.5197 - val_acc: 0.0086\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 10/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 5.0568 - acc: 0.0398\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.02050 to 0.03980, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:010-val_acc:0.040.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 207s 4s/step - loss: 4.0859 - acc: 0.1600 - val_loss: 5.0568 - val_acc: 0.0398\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 11/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 4.8528 - acc: 0.0637\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.03980 to 0.06370, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:011-val_acc:0.064.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 180s 4s/step - loss: 3.9537 - acc: 0.1816 - val_loss: 4.8528 - val_acc: 0.0637\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 12/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 5.1543 - acc: 0.0338\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.06370\n",
            "49/49 [==============================] - 155s 3s/step - loss: 3.7275 - acc: 0.2206 - val_loss: 5.1543 - val_acc: 0.0338\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 13/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 4.9226 - acc: 0.0690\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.06370 to 0.06900, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:013-val_acc:0.069.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 209s 4s/step - loss: 3.7286 - acc: 0.2251 - val_loss: 4.9226 - val_acc: 0.0690\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 14/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 5.0147 - acc: 0.0721\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.06900 to 0.07210, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:014-val_acc:0.072.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 183s 4s/step - loss: 3.5383 - acc: 0.2596 - val_loss: 5.0147 - val_acc: 0.0721\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 15/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 4.5585 - acc: 0.1491\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.07210 to 0.14910, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:015-val_acc:0.149.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 184s 4s/step - loss: 3.4900 - acc: 0.2745 - val_loss: 4.5585 - val_acc: 0.1491\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 16/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 4.2005 - acc: 0.2202\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.14910 to 0.22020, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:016-val_acc:0.220.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 182s 4s/step - loss: 3.3638 - acc: 0.3013 - val_loss: 4.2005 - val_acc: 0.2202\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 17/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 4.0157 - acc: 0.2533\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.22020 to 0.25330, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:017-val_acc:0.253.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 184s 4s/step - loss: 3.2365 - acc: 0.3259 - val_loss: 4.0157 - val_acc: 0.2533\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 18/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 4.6275 - acc: 0.1123\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.25330\n",
            "49/49 [==============================] - 152s 3s/step - loss: 3.0429 - acc: 0.3652 - val_loss: 4.6275 - val_acc: 0.1123\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 19/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 3.6797 - acc: 0.2819\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.25330 to 0.28190, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:019-val_acc:0.282.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 211s 4s/step - loss: 3.1425 - acc: 0.3519 - val_loss: 3.6797 - val_acc: 0.2819\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 20/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 3.4413 - acc: 0.3072\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.28190 to 0.30720, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:020-val_acc:0.307.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 185s 4s/step - loss: 2.9968 - acc: 0.3820 - val_loss: 3.4413 - val_acc: 0.3072\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 21/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 3.2658 - acc: 0.3381\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.30720 to 0.33810, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:021-val_acc:0.338.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 185s 4s/step - loss: 2.8678 - acc: 0.4074 - val_loss: 3.2658 - val_acc: 0.3381\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 22/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.9968 - acc: 0.3856\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.33810 to 0.38560, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:022-val_acc:0.386.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 185s 4s/step - loss: 2.8293 - acc: 0.4163 - val_loss: 2.9968 - val_acc: 0.3856\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 23/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 3.7923 - acc: 0.2586\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.38560\n",
            "49/49 [==============================] - 157s 3s/step - loss: 2.7248 - acc: 0.4389 - val_loss: 3.7923 - val_acc: 0.2586\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 24/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 4.4754 - acc: 0.1624\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.38560\n",
            "49/49 [==============================] - 184s 4s/step - loss: 2.6730 - acc: 0.4520 - val_loss: 4.4754 - val_acc: 0.1624\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 25/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 4.3438 - acc: 0.2093\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.38560\n",
            "49/49 [==============================] - 184s 4s/step - loss: 2.6423 - acc: 0.4595 - val_loss: 4.3438 - val_acc: 0.2093\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 26/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 5.3732 - acc: 0.1554\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.38560\n",
            "49/49 [==============================] - 184s 4s/step - loss: 2.6249 - acc: 0.4659 - val_loss: 5.3732 - val_acc: 0.1554\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 27/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 3.0962 - acc: 0.4099\n",
            "\n",
            "Epoch 00027: val_acc improved from 0.38560 to 0.40990, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:027-val_acc:0.410.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 211s 4s/step - loss: 2.6181 - acc: 0.4667 - val_loss: 3.0962 - val_acc: 0.4099\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 28/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.7674 - acc: 0.4462\n",
            "\n",
            "Epoch 00028: val_acc improved from 0.40990 to 0.44620, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:028-val_acc:0.446.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 184s 4s/step - loss: 2.5653 - acc: 0.4838 - val_loss: 2.7674 - val_acc: 0.4462\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 29/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.7996 - acc: 0.4391\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.44620\n",
            "49/49 [==============================] - 153s 3s/step - loss: 2.5018 - acc: 0.4962 - val_loss: 2.7996 - val_acc: 0.4391\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 30/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.7787 - acc: 0.4562\n",
            "\n",
            "Epoch 00030: val_acc improved from 0.44620 to 0.45620, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:030-val_acc:0.456.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 213s 4s/step - loss: 2.4628 - acc: 0.5045 - val_loss: 2.7787 - val_acc: 0.4562\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 31/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.9331 - acc: 0.4359\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.45620\n",
            "49/49 [==============================] - 157s 3s/step - loss: 2.4048 - acc: 0.5180 - val_loss: 2.9331 - val_acc: 0.4359\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 32/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.3207 - acc: 0.5360\n",
            "\n",
            "Epoch 00032: val_acc improved from 0.45620 to 0.53600, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:032-val_acc:0.536.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 9.999999747378752e-05\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 212s 4s/step - loss: 2.1688 - acc: 0.5744 - val_loss: 2.3207 - val_acc: 0.5360\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 33/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.2492 - acc: 0.5474\n",
            "\n",
            "Epoch 00033: val_acc improved from 0.53600 to 0.54740, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:033-val_acc:0.547.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 9.999999747378752e-05\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 187s 4s/step - loss: 2.0395 - acc: 0.6044 - val_loss: 2.2492 - val_acc: 0.5474\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 34/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.2178 - acc: 0.5510\n",
            "\n",
            "Epoch 00034: val_acc improved from 0.54740 to 0.55100, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:034-val_acc:0.551.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 9.999999747378752e-05\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 186s 4s/step - loss: 1.9829 - acc: 0.6158 - val_loss: 2.2178 - val_acc: 0.5510\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 35/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.2039 - acc: 0.5546\n",
            "\n",
            "Epoch 00035: val_acc improved from 0.55100 to 0.55460, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:035-val_acc:0.555.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 9.999999747378752e-05\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 186s 4s/step - loss: 1.9358 - acc: 0.6247 - val_loss: 2.2039 - val_acc: 0.5546\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 36/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1916 - acc: 0.5594\n",
            "\n",
            "Epoch 00036: val_acc improved from 0.55460 to 0.55940, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:036-val_acc:0.559.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 9.999999747378752e-05\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 191s 4s/step - loss: 1.9039 - acc: 0.6302 - val_loss: 2.1916 - val_acc: 0.5594\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 37/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.1929 - acc: 0.5542\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.55940\n",
            "49/49 [==============================] - 164s 3s/step - loss: 1.8693 - acc: 0.6360 - val_loss: 2.1929 - val_acc: 0.5542\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 38/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1707 - acc: 0.5592\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.55940\n",
            "49/49 [==============================] - 190s 4s/step - loss: 1.8420 - acc: 0.6426 - val_loss: 2.1707 - val_acc: 0.5592\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 39/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.1531 - acc: 0.5623\n",
            "\n",
            "Epoch 00039: val_acc improved from 0.55940 to 0.56230, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:039-val_acc:0.562.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 9.999999747378752e-05\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 218s 4s/step - loss: 1.8125 - acc: 0.6481 - val_loss: 2.1531 - val_acc: 0.5623\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 40/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.1495 - acc: 0.5601\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.56230\n",
            "49/49 [==============================] - 156s 3s/step - loss: 1.7889 - acc: 0.6524 - val_loss: 2.1495 - val_acc: 0.5601\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 41/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.1453 - acc: 0.5600\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.56230\n",
            "49/49 [==============================] - 186s 4s/step - loss: 1.7637 - acc: 0.6567 - val_loss: 2.1453 - val_acc: 0.5600\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 42/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1286 - acc: 0.5623\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.56230\n",
            "49/49 [==============================] - 189s 4s/step - loss: 1.7460 - acc: 0.6589 - val_loss: 2.1286 - val_acc: 0.5623\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 43/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1316 - acc: 0.5613\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.56230\n",
            "49/49 [==============================] - 187s 4s/step - loss: 1.7231 - acc: 0.6635 - val_loss: 2.1316 - val_acc: 0.5613\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 44/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.1239 - acc: 0.5609\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.56230\n",
            "49/49 [==============================] - 187s 4s/step - loss: 1.7026 - acc: 0.6688 - val_loss: 2.1239 - val_acc: 0.5609\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 45/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1211 - acc: 0.5645\n",
            "\n",
            "Epoch 00045: val_acc improved from 0.56230 to 0.56450, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:045-val_acc:0.564.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 9.999999747378752e-05\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 221s 5s/step - loss: 1.6877 - acc: 0.6695 - val_loss: 2.1211 - val_acc: 0.5645\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 46/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.1102 - acc: 0.5643\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.56450\n",
            "49/49 [==============================] - 158s 3s/step - loss: 1.6633 - acc: 0.6752 - val_loss: 2.1102 - val_acc: 0.5643\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 47/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.1189 - acc: 0.5631\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.56450\n",
            "49/49 [==============================] - 188s 4s/step - loss: 1.6520 - acc: 0.6770 - val_loss: 2.1189 - val_acc: 0.5631\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 48/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.1289 - acc: 0.5658\n",
            "\n",
            "Epoch 00048: val_acc improved from 0.56450 to 0.56580, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:048-val_acc:0.566.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 9.999999747378752e-05\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 218s 4s/step - loss: 1.6369 - acc: 0.6795 - val_loss: 2.1289 - val_acc: 0.5658\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 49/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.1066 - acc: 0.5653\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.56580\n",
            "49/49 [==============================] - 159s 3s/step - loss: 1.6180 - acc: 0.6824 - val_loss: 2.1066 - val_acc: 0.5653\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 50/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1127 - acc: 0.5602\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.56580\n",
            "49/49 [==============================] - 184s 4s/step - loss: 1.5984 - acc: 0.6874 - val_loss: 2.1127 - val_acc: 0.5602\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 51/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0991 - acc: 0.5639\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.56580\n",
            "49/49 [==============================] - 187s 4s/step - loss: 1.5819 - acc: 0.6903 - val_loss: 2.0991 - val_acc: 0.5639\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 52/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0897 - acc: 0.5664\n",
            "\n",
            "Epoch 00052: val_acc improved from 0.56580 to 0.56640, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:052-val_acc:0.566.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 4.999999873689376e-05\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 218s 4s/step - loss: 1.5515 - acc: 0.6982 - val_loss: 2.0897 - val_acc: 0.5664\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 53/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0773 - acc: 0.5687\n",
            "\n",
            "Epoch 00053: val_acc improved from 0.56640 to 0.56870, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:053-val_acc:0.569.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 4.999999873689376e-05\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 185s 4s/step - loss: 1.5370 - acc: 0.7016 - val_loss: 2.0773 - val_acc: 0.5687\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 54/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0845 - acc: 0.5672\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.56870\n",
            "49/49 [==============================] - 157s 3s/step - loss: 1.5248 - acc: 0.7034 - val_loss: 2.0845 - val_acc: 0.5672\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 55/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0849 - acc: 0.5667\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.56870\n",
            "49/49 [==============================] - 185s 4s/step - loss: 1.5190 - acc: 0.7051 - val_loss: 2.0849 - val_acc: 0.5667\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 56/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0861 - acc: 0.5663\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.56870\n",
            "49/49 [==============================] - 188s 4s/step - loss: 1.5121 - acc: 0.7088 - val_loss: 2.0861 - val_acc: 0.5663\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 57/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0753 - acc: 0.5701\n",
            "\n",
            "Epoch 00057: val_acc improved from 0.56870 to 0.57010, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:057-val_acc:0.570.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 4.999999873689376e-05\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 221s 5s/step - loss: 1.4989 - acc: 0.7097 - val_loss: 2.0753 - val_acc: 0.5701\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 58/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0801 - acc: 0.5673\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.57010\n",
            "49/49 [==============================] - 152s 3s/step - loss: 1.4908 - acc: 0.7109 - val_loss: 2.0801 - val_acc: 0.5673\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 59/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0827 - acc: 0.5674\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.57010\n",
            "49/49 [==============================] - 183s 4s/step - loss: 1.4861 - acc: 0.7116 - val_loss: 2.0827 - val_acc: 0.5674\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 60/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0742 - acc: 0.5691\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.57010\n",
            "49/49 [==============================] - 181s 4s/step - loss: 1.4743 - acc: 0.7156 - val_loss: 2.0742 - val_acc: 0.5691\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 61/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0768 - acc: 0.5689\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.57010\n",
            "49/49 [==============================] - 185s 4s/step - loss: 1.4678 - acc: 0.7179 - val_loss: 2.0768 - val_acc: 0.5689\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 62/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0755 - acc: 0.5693\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.57010\n",
            "49/49 [==============================] - 186s 4s/step - loss: 1.4615 - acc: 0.7159 - val_loss: 2.0755 - val_acc: 0.5693\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 63/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0860 - acc: 0.5666\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.57010\n",
            "49/49 [==============================] - 185s 4s/step - loss: 1.4525 - acc: 0.7188 - val_loss: 2.0860 - val_acc: 0.5666\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 64/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0806 - acc: 0.5687\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.57010\n",
            "49/49 [==============================] - 185s 4s/step - loss: 1.4509 - acc: 0.7204 - val_loss: 2.0806 - val_acc: 0.5687\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 65/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0800 - acc: 0.5683\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.57010\n",
            "49/49 [==============================] - 183s 4s/step - loss: 1.4413 - acc: 0.7210 - val_loss: 2.0800 - val_acc: 0.5683\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 66/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0794 - acc: 0.5673\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.57010\n",
            "49/49 [==============================] - 182s 4s/step - loss: 1.4347 - acc: 0.7224 - val_loss: 2.0794 - val_acc: 0.5673\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 67/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0793 - acc: 0.5712\n",
            "\n",
            "Epoch 00067: val_acc improved from 0.57010 to 0.57120, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:067-val_acc:0.571.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 4.999999873689376e-05\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 218s 4s/step - loss: 1.4246 - acc: 0.7251 - val_loss: 2.0793 - val_acc: 0.5712\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 68/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0797 - acc: 0.5664\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.57120\n",
            "49/49 [==============================] - 151s 3s/step - loss: 1.4203 - acc: 0.7270 - val_loss: 2.0797 - val_acc: 0.5664\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 69/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0717 - acc: 0.5681\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.57120\n",
            "49/49 [==============================] - 184s 4s/step - loss: 1.4103 - acc: 0.7268 - val_loss: 2.0717 - val_acc: 0.5681\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 70/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0761 - acc: 0.5657\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.57120\n",
            "49/49 [==============================] - 181s 4s/step - loss: 1.4056 - acc: 0.7290 - val_loss: 2.0761 - val_acc: 0.5657\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 71/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0788 - acc: 0.5697\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.57120\n",
            "49/49 [==============================] - 184s 4s/step - loss: 1.3935 - acc: 0.7314 - val_loss: 2.0788 - val_acc: 0.5697\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 72/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0759 - acc: 0.5656\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.57120\n",
            "49/49 [==============================] - 183s 4s/step - loss: 1.3941 - acc: 0.7306 - val_loss: 2.0759 - val_acc: 0.5656\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 73/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0841 - acc: 0.5673\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.57120\n",
            "49/49 [==============================] - 185s 4s/step - loss: 1.3918 - acc: 0.7322 - val_loss: 2.0841 - val_acc: 0.5673\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 74/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0752 - acc: 0.5670\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.57120\n",
            "49/49 [==============================] - 181s 4s/step - loss: 1.3848 - acc: 0.7326 - val_loss: 2.0752 - val_acc: 0.5670\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 75/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0706 - acc: 0.5688\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.57120\n",
            "49/49 [==============================] - 185s 4s/step - loss: 1.3791 - acc: 0.7337 - val_loss: 2.0706 - val_acc: 0.5688\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 76/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0804 - acc: 0.5682\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.57120\n",
            "49/49 [==============================] - 193s 4s/step - loss: 1.3728 - acc: 0.7345 - val_loss: 2.0804 - val_acc: 0.5682\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 77/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0616 - acc: 0.5697\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 0.57120\n",
            "\n",
            "Epoch 00077: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "49/49 [==============================] - 191s 4s/step - loss: 1.3504 - acc: 0.7399 - val_loss: 2.0616 - val_acc: 0.5697\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 78/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0580 - acc: 0.5712\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.57120\n",
            "49/49 [==============================] - 190s 4s/step - loss: 1.3454 - acc: 0.7410 - val_loss: 2.0580 - val_acc: 0.5712\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 79/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0619 - acc: 0.5720\n",
            "\n",
            "Epoch 00079: val_acc improved from 0.57120 to 0.57200, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:079-val_acc:0.572.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 9.999999747378752e-06\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 220s 4s/step - loss: 1.3437 - acc: 0.7434 - val_loss: 2.0619 - val_acc: 0.5720\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 80/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0602 - acc: 0.5725\n",
            "\n",
            "Epoch 00080: val_acc improved from 0.57200 to 0.57250, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:080-val_acc:0.572.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 9.999999747378752e-06\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 191s 4s/step - loss: 1.3326 - acc: 0.7449 - val_loss: 2.0602 - val_acc: 0.5725\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 81/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0628 - acc: 0.5721\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.57250\n",
            "49/49 [==============================] - 156s 3s/step - loss: 1.3394 - acc: 0.7439 - val_loss: 2.0628 - val_acc: 0.5721\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 82/125\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.0607 - acc: 0.5716\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.57250\n",
            "49/49 [==============================] - 191s 4s/step - loss: 1.3422 - acc: 0.7426 - val_loss: 2.0607 - val_acc: 0.5716\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 83/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0602 - acc: 0.5703\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.57250\n",
            "49/49 [==============================] - 190s 4s/step - loss: 1.3320 - acc: 0.7461 - val_loss: 2.0602 - val_acc: 0.5703\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 84/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0634 - acc: 0.5706\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.57250\n",
            "49/49 [==============================] - 192s 4s/step - loss: 1.3294 - acc: 0.7453 - val_loss: 2.0634 - val_acc: 0.5706\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 85/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0633 - acc: 0.5731\n",
            "\n",
            "Epoch 00085: val_acc improved from 0.57250 to 0.57310, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:085-val_acc:0.573.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 9.999999747378752e-06\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 222s 5s/step - loss: 1.3253 - acc: 0.7475 - val_loss: 2.0633 - val_acc: 0.5731\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 86/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0660 - acc: 0.5735\n",
            "\n",
            "Epoch 00086: val_acc improved from 0.57310 to 0.57350, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:086-val_acc:0.573.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 9.999999747378752e-06\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 197s 4s/step - loss: 1.3319 - acc: 0.7444 - val_loss: 2.0660 - val_acc: 0.5735\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 87/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0635 - acc: 0.5724\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 158s 3s/step - loss: 1.3309 - acc: 0.7447 - val_loss: 2.0635 - val_acc: 0.5724\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 88/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0642 - acc: 0.5725\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 191s 4s/step - loss: 1.3273 - acc: 0.7451 - val_loss: 2.0642 - val_acc: 0.5725\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 89/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0626 - acc: 0.5724\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 193s 4s/step - loss: 1.3204 - acc: 0.7473 - val_loss: 2.0626 - val_acc: 0.5724\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 90/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0616 - acc: 0.5715\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.57350\n",
            "\n",
            "Epoch 00090: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "49/49 [==============================] - 192s 4s/step - loss: 1.3228 - acc: 0.7474 - val_loss: 2.0616 - val_acc: 0.5715\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 91/125\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.0629 - acc: 0.5724\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 196s 4s/step - loss: 1.3241 - acc: 0.7467 - val_loss: 2.0629 - val_acc: 0.5724\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 92/125\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0658 - acc: 0.5711\n",
            "\n",
            "Epoch 00092: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 189s 4s/step - loss: 1.3224 - acc: 0.7466 - val_loss: 2.0658 - val_acc: 0.5711\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 93/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0620 - acc: 0.5730\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 190s 4s/step - loss: 1.3183 - acc: 0.7482 - val_loss: 2.0620 - val_acc: 0.5730\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 94/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0647 - acc: 0.5712\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 193s 4s/step - loss: 1.3178 - acc: 0.7480 - val_loss: 2.0647 - val_acc: 0.5712\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 95/125\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.0607 - acc: 0.5727\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 192s 4s/step - loss: 1.3135 - acc: 0.7498 - val_loss: 2.0607 - val_acc: 0.5727\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 96/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0600 - acc: 0.5722\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 190s 4s/step - loss: 1.3136 - acc: 0.7479 - val_loss: 2.0600 - val_acc: 0.5722\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 97/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0608 - acc: 0.5715\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 194s 4s/step - loss: 1.3176 - acc: 0.7489 - val_loss: 2.0608 - val_acc: 0.5715\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 98/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0619 - acc: 0.5704\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 193s 4s/step - loss: 1.3203 - acc: 0.7464 - val_loss: 2.0619 - val_acc: 0.5704\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 99/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0624 - acc: 0.5701\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 193s 4s/step - loss: 1.3105 - acc: 0.7507 - val_loss: 2.0624 - val_acc: 0.5701\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 100/125\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.0605 - acc: 0.5725\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.57350\n",
            "\n",
            "Epoch 00100: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "49/49 [==============================] - 197s 4s/step - loss: 1.3110 - acc: 0.7495 - val_loss: 2.0605 - val_acc: 0.5725\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 101/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0618 - acc: 0.5713\n",
            "\n",
            "Epoch 00101: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 190s 4s/step - loss: 1.3094 - acc: 0.7507 - val_loss: 2.0618 - val_acc: 0.5713\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 102/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0600 - acc: 0.5719\n",
            "\n",
            "Epoch 00102: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 194s 4s/step - loss: 1.2997 - acc: 0.7524 - val_loss: 2.0600 - val_acc: 0.5719\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 103/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0608 - acc: 0.5714\n",
            "\n",
            "Epoch 00103: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 191s 4s/step - loss: 1.3125 - acc: 0.7493 - val_loss: 2.0608 - val_acc: 0.5714\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 104/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0609 - acc: 0.5711\n",
            "\n",
            "Epoch 00104: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 192s 4s/step - loss: 1.3039 - acc: 0.7529 - val_loss: 2.0609 - val_acc: 0.5711\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 105/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0614 - acc: 0.5719\n",
            "\n",
            "Epoch 00105: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 193s 4s/step - loss: 1.3055 - acc: 0.7498 - val_loss: 2.0614 - val_acc: 0.5719\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 106/125\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.0630 - acc: 0.5716\n",
            "\n",
            "Epoch 00106: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 194s 4s/step - loss: 1.3066 - acc: 0.7489 - val_loss: 2.0630 - val_acc: 0.5716\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 107/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0593 - acc: 0.5719\n",
            "\n",
            "Epoch 00107: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 191s 4s/step - loss: 1.3020 - acc: 0.7511 - val_loss: 2.0593 - val_acc: 0.5719\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 108/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0601 - acc: 0.5716\n",
            "\n",
            "Epoch 00108: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 195s 4s/step - loss: 1.2965 - acc: 0.7541 - val_loss: 2.0601 - val_acc: 0.5716\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 109/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0595 - acc: 0.5720\n",
            "\n",
            "Epoch 00109: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 191s 4s/step - loss: 1.3064 - acc: 0.7497 - val_loss: 2.0595 - val_acc: 0.5720\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 110/125\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.0604 - acc: 0.5711\n",
            "\n",
            "Epoch 00110: val_acc did not improve from 0.57350\n",
            "\n",
            "Epoch 00110: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-07.\n",
            "49/49 [==============================] - 194s 4s/step - loss: 1.3026 - acc: 0.7515 - val_loss: 2.0604 - val_acc: 0.5711\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 111/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0628 - acc: 0.5714\n",
            "\n",
            "Epoch 00111: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 191s 4s/step - loss: 1.2958 - acc: 0.7527 - val_loss: 2.0628 - val_acc: 0.5714\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 112/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0606 - acc: 0.5708\n",
            "\n",
            "Epoch 00112: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 195s 4s/step - loss: 1.2968 - acc: 0.7518 - val_loss: 2.0606 - val_acc: 0.5708\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 113/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0593 - acc: 0.5708\n",
            "\n",
            "Epoch 00113: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 198s 4s/step - loss: 1.3001 - acc: 0.7529 - val_loss: 2.0593 - val_acc: 0.5708\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 114/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0598 - acc: 0.5706\n",
            "\n",
            "Epoch 00114: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 193s 4s/step - loss: 1.2958 - acc: 0.7524 - val_loss: 2.0598 - val_acc: 0.5706\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 115/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0573 - acc: 0.5703\n",
            "\n",
            "Epoch 00115: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 193s 4s/step - loss: 1.2970 - acc: 0.7513 - val_loss: 2.0573 - val_acc: 0.5703\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 116/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0599 - acc: 0.5711\n",
            "\n",
            "Epoch 00116: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 195s 4s/step - loss: 1.2953 - acc: 0.7536 - val_loss: 2.0599 - val_acc: 0.5711\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 117/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0617 - acc: 0.5723\n",
            "\n",
            "Epoch 00117: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 195s 4s/step - loss: 1.2908 - acc: 0.7554 - val_loss: 2.0617 - val_acc: 0.5723\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 118/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0597 - acc: 0.5718\n",
            "\n",
            "Epoch 00118: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 196s 4s/step - loss: 1.2968 - acc: 0.7518 - val_loss: 2.0597 - val_acc: 0.5718\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 119/125\n",
            "5/5 [==============================] - 15s 3s/step - loss: 2.0555 - acc: 0.5712\n",
            "\n",
            "Epoch 00119: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 197s 4s/step - loss: 1.2872 - acc: 0.7548 - val_loss: 2.0555 - val_acc: 0.5712\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 120/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0576 - acc: 0.5700\n",
            "\n",
            "Epoch 00120: val_acc did not improve from 0.57350\n",
            "\n",
            "Epoch 00120: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-07.\n",
            "49/49 [==============================] - 195s 4s/step - loss: 1.2905 - acc: 0.7539 - val_loss: 2.0576 - val_acc: 0.5700\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 121/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0605 - acc: 0.5717\n",
            "\n",
            "Epoch 00121: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 196s 4s/step - loss: 1.2894 - acc: 0.7550 - val_loss: 2.0605 - val_acc: 0.5717\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 122/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0613 - acc: 0.5717\n",
            "\n",
            "Epoch 00122: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 195s 4s/step - loss: 1.2924 - acc: 0.7514 - val_loss: 2.0613 - val_acc: 0.5717\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 123/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0627 - acc: 0.5717\n",
            "\n",
            "Epoch 00123: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 196s 4s/step - loss: 1.2880 - acc: 0.7537 - val_loss: 2.0627 - val_acc: 0.5717\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 124/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0591 - acc: 0.5704\n",
            "\n",
            "Epoch 00124: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 195s 4s/step - loss: 1.2892 - acc: 0.7557 - val_loss: 2.0591 - val_acc: 0.5704\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 125/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0573 - acc: 0.5701\n",
            "\n",
            "Epoch 00125: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 190s 4s/step - loss: 1.2887 - acc: 0.7518 - val_loss: 2.0573 - val_acc: 0.5701\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4557032080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax3hHSJWQZvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq = iaa.Sequential(\n",
        "      [\n",
        "      iaa.Sometimes(0.5, iaa.CoarseDropout((0.5), size_percent=(0.01))),\n",
        "\t\t  iaa.Sometimes(0.5, iaa.Crop(percent=(0, 0.6))),\n",
        "\t\t  iaa.Sometimes(0.4, iaa.Affine(scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}, rotate=(-30, 30))),\t  \n",
        "      iaa.Sometimes(0.2, iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5))),\n",
        "      iaa.Sometimes(0.1, iaa.OneOf([\n",
        "                    iaa.EdgeDetect(alpha=(0, 0.7)),\n",
        "                    iaa.DirectedEdgeDetect(alpha=(0, 0.7), direction=(0.0, 1.0)),\n",
        "                    ])),\n",
        "      iaa.Sometimes(0.4, iaa.AdditiveGaussianNoise(scale=(0.1))),\n",
        "      iaa.Sometimes(0.1, iaa.GaussianBlur(sigma=(0, 0.5)))\n",
        "\t  ],\n",
        "\n",
        "      # do all of the above augmentations in random order\n",
        "      random_order=True\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXPkwYbrZX6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale= 1./255,\n",
        "    preprocessing_function = seq.augment_image\n",
        "    )\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY_5HfUaRgNu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    #Learning Rate Schedule\n",
        "    #\n",
        "    #Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    #Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "    #    epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "    #    lr (float32): learning rate\n",
        "    #\n",
        "    \n",
        "    if epoch > 15:\n",
        "        lr = 0.5e-5\n",
        "    elif epoch > 5:\n",
        "        lr = 1e-5\n",
        "    else:\n",
        "        lr = 0.5e-4\n",
        "    print('Learning rate (from LearningRateScheduler): ', lr)\n",
        "    return lr\n",
        "\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BroQd0YcQ4a_",
        "colab_type": "code",
        "outputId": "7ad4f1e8-24e2-4fc6-c571-6c16de383ab2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2108
        }
      },
      "source": [
        "tpu_model.fit_generator(train_generator, \n",
        "                        epochs=20, \n",
        "                        steps_per_epoch=int(100000//batch_size),\n",
        "                        validation_steps=int(10000//batch_size), \n",
        "                        validation_data=validation_generator,\n",
        "                        callbacks=[checkpoint, lr_reducer, csv_logger, lr_scheduler] )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 1/20\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.0973 - acc: 0.5632\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 200s 4s/step - loss: 1.3079 - acc: 0.7484 - val_loss: 2.0973 - val_acc: 0.5632\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 2/20\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.0778 - acc: 0.5696\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 194s 4s/step - loss: 1.3057 - acc: 0.7476 - val_loss: 2.0778 - val_acc: 0.5696\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 3/20\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.0822 - acc: 0.5642\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 192s 4s/step - loss: 1.3009 - acc: 0.7484 - val_loss: 2.0822 - val_acc: 0.5642\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 4/20\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0837 - acc: 0.5642\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 198s 4s/step - loss: 1.2959 - acc: 0.7497 - val_loss: 2.0837 - val_acc: 0.5642\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 5/20\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.0808 - acc: 0.5654\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 195s 4s/step - loss: 1.2960 - acc: 0.7485 - val_loss: 2.0808 - val_acc: 0.5654\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 6/20\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.0868 - acc: 0.5666\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 194s 4s/step - loss: 1.2947 - acc: 0.7495 - val_loss: 2.0868 - val_acc: 0.5666\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 7/20\n",
            "5/5 [==============================] - 15s 3s/step - loss: 2.0621 - acc: 0.5689\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 196s 4s/step - loss: 1.2667 - acc: 0.7580 - val_loss: 2.0621 - val_acc: 0.5689\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 8/20\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0590 - acc: 0.5694\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 191s 4s/step - loss: 1.2638 - acc: 0.7575 - val_loss: 2.0590 - val_acc: 0.5694\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 9/20\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0593 - acc: 0.5714\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 194s 4s/step - loss: 1.2604 - acc: 0.7593 - val_loss: 2.0593 - val_acc: 0.5714\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 10/20\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0637 - acc: 0.5702\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 192s 4s/step - loss: 1.2616 - acc: 0.7590 - val_loss: 2.0637 - val_acc: 0.5702\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 11/20\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.0700 - acc: 0.5687\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 193s 4s/step - loss: 1.2616 - acc: 0.7579 - val_loss: 2.0700 - val_acc: 0.5687\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 12/20\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.0688 - acc: 0.5704\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 197s 4s/step - loss: 1.2577 - acc: 0.7592 - val_loss: 2.0688 - val_acc: 0.5704\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 13/20\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.0628 - acc: 0.5704\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 194s 4s/step - loss: 1.2558 - acc: 0.7597 - val_loss: 2.0628 - val_acc: 0.5704\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 14/20\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.0604 - acc: 0.5685\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 196s 4s/step - loss: 1.2590 - acc: 0.7585 - val_loss: 2.0604 - val_acc: 0.5685\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 15/20\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0646 - acc: 0.5693\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 196s 4s/step - loss: 1.2535 - acc: 0.7598 - val_loss: 2.0646 - val_acc: 0.5693\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 16/20\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0610 - acc: 0.5689\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 194s 4s/step - loss: 1.2585 - acc: 0.7591 - val_loss: 2.0610 - val_acc: 0.5689\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 17/20\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0631 - acc: 0.5703\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 192s 4s/step - loss: 1.2511 - acc: 0.7615 - val_loss: 2.0631 - val_acc: 0.5703\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 18/20\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.0633 - acc: 0.5698\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 195s 4s/step - loss: 1.2487 - acc: 0.7619 - val_loss: 2.0633 - val_acc: 0.5698\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 19/20\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0625 - acc: 0.5713\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.57350\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-07.\n",
            "49/49 [==============================] - 192s 4s/step - loss: 1.2460 - acc: 0.7611 - val_loss: 2.0625 - val_acc: 0.5713\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 20/20\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0640 - acc: 0.5710\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 193s 4s/step - loss: 1.2494 - acc: 0.7607 - val_loss: 2.0640 - val_acc: 0.5710\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4555987940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsSwecQZ5DPP",
        "colab_type": "code",
        "outputId": "03d22cc5-389a-49a6-f11b-db89ee133473",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "tpu_model.save_weights('/content/gdrive/My Drive/savedWeightsFinal.hdf5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 4.999999873689376e-06\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-6-6yLr5eg2",
        "colab_type": "code",
        "outputId": "6db9b484-0685-4069-a97d-983c0374fa44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "validation_generator = valid_datagen.flow_from_dataframe(val_data, directory='./tinyimagenet/val/images/', x_col='File', y_col='Class', target_size=(64, 64),\n",
        "                                                    color_mode='rgb', class_mode='categorical', batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10000 images belonging to 200 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeTcnnzGaUsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "steps=np.math.ceil(validation_generator.samples/validation_generator.batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVxNHsi3slOU",
        "colab_type": "code",
        "outputId": "fdc6ba35-fc19-43c9-f521-7c08b72eb309",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "Y_pred = tpu_model.predict_generator(validation_generator, steps)\n",
        "y_pred = np.argmax(Y_pred, axis=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=infer (# of cores 8), [TensorSpec(shape=(256, 64, 64, 3), dtype=tf.float32, name='input_1_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 4.999999873689376e-06, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 18.859996557235718 secs\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=infer (# of cores 8), [TensorSpec(shape=(226, 64, 64, 3), dtype=tf.float32, name='input_1_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 12.929059028625488 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW19xRgXbklv",
        "colab_type": "code",
        "outputId": "f34affc2-fed1-48c8-b787-a3306ea68529",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3536
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(validation_generator.classes, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.86        50\n",
            "           1       0.79      0.76      0.78        50\n",
            "           2       0.62      0.56      0.59        50\n",
            "           3       0.42      0.40      0.41        50\n",
            "           4       0.51      0.56      0.53        50\n",
            "           5       0.47      0.40      0.43        50\n",
            "           6       0.67      0.68      0.67        50\n",
            "           7       0.59      0.48      0.53        50\n",
            "           8       0.81      0.78      0.80        50\n",
            "           9       0.57      0.54      0.56        50\n",
            "          10       0.47      0.52      0.50        50\n",
            "          11       0.64      0.60      0.62        50\n",
            "          12       0.72      0.72      0.72        50\n",
            "          13       0.69      0.76      0.72        50\n",
            "          14       0.69      0.72      0.71        50\n",
            "          15       0.59      0.48      0.53        50\n",
            "          16       0.42      0.38      0.40        50\n",
            "          17       0.67      0.72      0.69        50\n",
            "          18       0.53      0.56      0.54        50\n",
            "          19       0.62      0.66      0.64        50\n",
            "          20       0.72      0.68      0.70        50\n",
            "          21       0.89      0.66      0.76        50\n",
            "          22       0.65      0.66      0.65        50\n",
            "          23       0.74      0.90      0.81        50\n",
            "          24       0.55      0.48      0.51        50\n",
            "          25       0.86      0.74      0.80        50\n",
            "          26       0.53      0.58      0.55        50\n",
            "          27       0.41      0.32      0.36        50\n",
            "          28       0.60      0.50      0.54        50\n",
            "          29       0.38      0.34      0.36        50\n",
            "          30       0.55      0.52      0.54        50\n",
            "          31       0.73      0.64      0.68        50\n",
            "          32       0.42      0.38      0.40        50\n",
            "          33       0.60      0.50      0.54        50\n",
            "          34       0.79      0.74      0.76        50\n",
            "          35       0.57      0.68      0.62        50\n",
            "          36       0.82      0.80      0.81        50\n",
            "          37       0.58      0.60      0.59        50\n",
            "          38       0.66      0.66      0.66        50\n",
            "          39       0.48      0.58      0.52        50\n",
            "          40       0.42      0.40      0.41        50\n",
            "          41       0.47      0.46      0.46        50\n",
            "          42       0.50      0.42      0.46        50\n",
            "          43       0.45      0.50      0.48        50\n",
            "          44       0.93      0.86      0.90        50\n",
            "          45       0.85      0.78      0.81        50\n",
            "          46       0.51      0.62      0.56        50\n",
            "          47       0.67      0.62      0.65        50\n",
            "          48       0.41      0.32      0.36        50\n",
            "          49       0.45      0.50      0.48        50\n",
            "          50       0.76      0.76      0.76        50\n",
            "          51       0.57      0.58      0.57        50\n",
            "          52       0.75      0.76      0.75        50\n",
            "          53       0.64      0.78      0.70        50\n",
            "          54       0.72      0.62      0.67        50\n",
            "          55       0.71      0.68      0.69        50\n",
            "          56       0.61      0.50      0.55        50\n",
            "          57       0.81      0.70      0.75        50\n",
            "          58       0.87      0.78      0.82        50\n",
            "          59       0.53      0.46      0.49        50\n",
            "          60       0.72      0.72      0.72        50\n",
            "          61       0.60      0.58      0.59        50\n",
            "          62       0.38      0.40      0.39        50\n",
            "          63       0.49      0.58      0.53        50\n",
            "          64       0.26      0.28      0.27        50\n",
            "          65       0.43      0.44      0.44        50\n",
            "          66       0.61      0.60      0.61        50\n",
            "          67       0.47      0.40      0.43        50\n",
            "          68       0.69      0.58      0.63        50\n",
            "          69       0.41      0.50      0.45        50\n",
            "          70       0.66      0.54      0.59        50\n",
            "          71       0.77      0.80      0.78        50\n",
            "          72       0.47      0.44      0.45        50\n",
            "          73       0.46      0.54      0.50        50\n",
            "          74       0.69      0.68      0.69        50\n",
            "          75       0.43      0.54      0.48        50\n",
            "          76       0.59      0.52      0.55        50\n",
            "          77       0.35      0.34      0.35        50\n",
            "          78       0.65      0.72      0.69        50\n",
            "          79       0.53      0.38      0.44        50\n",
            "          80       0.27      0.26      0.27        50\n",
            "          81       0.82      0.90      0.86        50\n",
            "          82       0.70      0.62      0.66        50\n",
            "          83       0.62      0.48      0.54        50\n",
            "          84       0.47      0.40      0.43        50\n",
            "          85       0.51      0.44      0.47        50\n",
            "          86       0.53      0.56      0.54        50\n",
            "          87       0.40      0.34      0.37        50\n",
            "          88       0.33      0.36      0.35        50\n",
            "          89       0.53      0.50      0.52        50\n",
            "          90       0.64      0.70      0.67        50\n",
            "          91       0.70      0.64      0.67        50\n",
            "          92       0.71      0.64      0.67        50\n",
            "          93       0.59      0.66      0.62        50\n",
            "          94       0.52      0.54      0.53        50\n",
            "          95       0.63      0.54      0.58        50\n",
            "          96       0.52      0.52      0.52        50\n",
            "          97       0.50      0.58      0.54        50\n",
            "          98       0.62      0.64      0.63        50\n",
            "          99       0.27      0.34      0.30        50\n",
            "         100       0.22      0.24      0.23        50\n",
            "         101       0.74      0.64      0.69        50\n",
            "         102       0.57      0.62      0.60        50\n",
            "         103       0.78      0.84      0.81        50\n",
            "         104       0.41      0.38      0.40        50\n",
            "         105       0.38      0.36      0.37        50\n",
            "         106       0.46      0.44      0.45        50\n",
            "         107       0.65      0.78      0.71        50\n",
            "         108       0.78      0.86      0.82        50\n",
            "         109       0.60      0.60      0.60        50\n",
            "         110       0.58      0.56      0.57        50\n",
            "         111       0.65      0.68      0.67        50\n",
            "         112       0.37      0.38      0.37        50\n",
            "         113       0.42      0.54      0.47        50\n",
            "         114       0.46      0.42      0.44        50\n",
            "         115       0.88      0.84      0.86        50\n",
            "         116       0.64      0.58      0.61        50\n",
            "         117       0.70      0.60      0.65        50\n",
            "         118       0.76      0.78      0.77        50\n",
            "         119       0.40      0.42      0.41        50\n",
            "         120       0.48      0.44      0.46        50\n",
            "         121       0.65      0.62      0.63        50\n",
            "         122       0.28      0.34      0.31        50\n",
            "         123       0.42      0.44      0.43        50\n",
            "         124       0.71      0.82      0.76        50\n",
            "         125       0.35      0.34      0.34        50\n",
            "         126       0.65      0.64      0.65        50\n",
            "         127       0.63      0.58      0.60        50\n",
            "         128       0.48      0.58      0.53        50\n",
            "         129       0.76      0.74      0.75        50\n",
            "         130       0.42      0.42      0.42        50\n",
            "         131       0.15      0.14      0.14        50\n",
            "         132       0.22      0.24      0.23        50\n",
            "         133       0.85      0.78      0.81        50\n",
            "         134       0.41      0.48      0.44        50\n",
            "         135       0.32      0.26      0.29        50\n",
            "         136       0.50      0.60      0.55        50\n",
            "         137       0.40      0.32      0.36        50\n",
            "         138       0.32      0.34      0.33        50\n",
            "         139       0.40      0.34      0.37        50\n",
            "         140       0.74      0.62      0.67        50\n",
            "         141       0.52      0.46      0.49        50\n",
            "         142       0.55      0.48      0.51        50\n",
            "         143       0.86      0.84      0.85        50\n",
            "         144       0.47      0.54      0.50        50\n",
            "         145       0.80      0.90      0.85        50\n",
            "         146       0.74      0.70      0.72        50\n",
            "         147       0.43      0.52      0.47        50\n",
            "         148       0.60      0.48      0.53        50\n",
            "         149       0.57      0.70      0.63        50\n",
            "         150       0.60      0.54      0.57        50\n",
            "         151       0.48      0.56      0.52        50\n",
            "         152       0.58      0.68      0.62        50\n",
            "         153       0.56      0.60      0.58        50\n",
            "         154       0.54      0.50      0.52        50\n",
            "         155       0.58      0.60      0.59        50\n",
            "         156       0.59      0.54      0.56        50\n",
            "         157       0.62      0.56      0.59        50\n",
            "         158       0.47      0.38      0.42        50\n",
            "         159       0.23      0.26      0.25        50\n",
            "         160       0.41      0.44      0.42        50\n",
            "         161       0.58      0.66      0.62        50\n",
            "         162       0.64      0.64      0.64        50\n",
            "         163       0.69      0.62      0.65        50\n",
            "         164       0.55      0.58      0.56        50\n",
            "         165       0.95      0.80      0.87        50\n",
            "         166       0.81      0.84      0.82        50\n",
            "         167       0.52      0.54      0.53        50\n",
            "         168       0.28      0.26      0.27        50\n",
            "         169       0.57      0.62      0.60        50\n",
            "         170       0.75      0.82      0.78        50\n",
            "         171       0.63      0.78      0.70        50\n",
            "         172       0.30      0.36      0.33        50\n",
            "         173       0.80      0.80      0.80        50\n",
            "         174       0.47      0.58      0.52        50\n",
            "         175       0.27      0.22      0.24        50\n",
            "         176       0.81      0.78      0.80        50\n",
            "         177       0.42      0.50      0.45        50\n",
            "         178       0.65      0.68      0.67        50\n",
            "         179       0.37      0.40      0.38        50\n",
            "         180       0.38      0.40      0.39        50\n",
            "         181       0.71      0.54      0.61        50\n",
            "         182       0.58      0.52      0.55        50\n",
            "         183       0.64      0.56      0.60        50\n",
            "         184       0.80      0.74      0.77        50\n",
            "         185       0.71      0.72      0.71        50\n",
            "         186       0.70      0.64      0.67        50\n",
            "         187       0.66      0.66      0.66        50\n",
            "         188       0.67      0.62      0.65        50\n",
            "         189       0.82      0.80      0.81        50\n",
            "         190       0.47      0.54      0.50        50\n",
            "         191       0.77      0.74      0.76        50\n",
            "         192       0.70      0.60      0.65        50\n",
            "         193       0.75      0.84      0.79        50\n",
            "         194       0.65      0.68      0.67        50\n",
            "         195       0.44      0.60      0.51        50\n",
            "         196       0.60      0.74      0.66        50\n",
            "         197       0.33      0.40      0.36        50\n",
            "         198       0.45      0.56      0.50        50\n",
            "         199       0.52      0.50      0.51        50\n",
            "\n",
            "   micro avg       0.57      0.57      0.57     10000\n",
            "   macro avg       0.58      0.57      0.57     10000\n",
            "weighted avg       0.58      0.57      0.57     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWKf1xYWRh-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd \n",
        "pd.DataFrame(y_pred).to_csv(\"predFinal.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0Z9tKe5Rrmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_dict = validation_generator.class_indices\n",
        "\n",
        "import csv\n",
        "\n",
        "\n",
        "with open('indicesFinal.csv', 'w') as f:  # Just use 'w' mode in 3.x\n",
        "    w = csv.DictWriter(f, my_dict.keys())\n",
        "    w.writeheader()\n",
        "    w.writerow(my_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW3VDCPfOllV",
        "colab_type": "code",
        "outputId": "b78f359f-0c52-4dd0-c57c-a637695ac477",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory( r'./tinyimagenet/train/', target_size=(64, 64), color_mode='rgb', \n",
        "                                                    batch_size=batch_size, class_mode='categorical', shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P74lqVZG9ury",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y1_pred = tpu_model.predict_generator(train_generator, np.math.ceil(train_generator.samples/train_generator.batch_size))\n",
        "y1_pred = np.argmax(Y1_pred, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzPt72-G-CFl",
        "colab_type": "code",
        "outputId": "9001cfdd-62f7-4cb3-821d-b8aabdba42a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3536
        }
      },
      "source": [
        "print(classification_report(train_generator.classes, y1_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.57      0.49       500\n",
            "           1       0.85      0.46      0.59       500\n",
            "           2       0.37      0.41      0.39       500\n",
            "           3       0.34      0.22      0.26       500\n",
            "           4       0.53      0.22      0.32       500\n",
            "           5       0.63      0.16      0.26       500\n",
            "           6       0.52      0.28      0.36       500\n",
            "           7       0.46      0.21      0.29       500\n",
            "           8       0.59      0.35      0.44       500\n",
            "           9       0.60      0.18      0.27       500\n",
            "          10       0.36      0.26      0.30       500\n",
            "          11       0.44      0.30      0.36       500\n",
            "          12       0.52      0.41      0.46       500\n",
            "          13       0.64      0.48      0.55       500\n",
            "          14       0.58      0.38      0.46       500\n",
            "          15       0.38      0.29      0.33       500\n",
            "          16       0.35      0.33      0.34       500\n",
            "          17       0.28      0.44      0.34       500\n",
            "          18       0.30      0.46      0.36       500\n",
            "          19       0.31      0.39      0.35       500\n",
            "          20       0.27      0.39      0.32       500\n",
            "          21       0.77      0.46      0.57       500\n",
            "          22       0.35      0.46      0.39       500\n",
            "          23       0.57      0.59      0.58       500\n",
            "          24       0.50      0.23      0.32       500\n",
            "          25       0.59      0.35      0.44       500\n",
            "          26       0.45      0.32      0.38       500\n",
            "          27       0.40      0.24      0.30       500\n",
            "          28       0.57      0.31      0.40       500\n",
            "          29       0.45      0.20      0.28       500\n",
            "          30       0.69      0.28      0.40       500\n",
            "          31       0.32      0.36      0.34       500\n",
            "          32       0.56      0.27      0.36       500\n",
            "          33       0.46      0.24      0.32       500\n",
            "          34       0.48      0.29      0.36       500\n",
            "          35       0.44      0.35      0.39       500\n",
            "          36       0.55      0.47      0.51       500\n",
            "          37       0.51      0.37      0.43       500\n",
            "          38       0.58      0.38      0.46       500\n",
            "          39       0.34      0.40      0.36       500\n",
            "          40       0.52      0.28      0.36       500\n",
            "          41       0.24      0.37      0.29       500\n",
            "          42       0.28      0.33      0.30       500\n",
            "          43       0.16      0.39      0.23       500\n",
            "          44       0.41      0.64      0.50       500\n",
            "          45       0.28      0.63      0.39       500\n",
            "          46       0.42      0.33      0.37       500\n",
            "          47       0.44      0.37      0.40       500\n",
            "          48       0.41      0.26      0.31       500\n",
            "          49       0.62      0.27      0.38       500\n",
            "          50       0.58      0.39      0.47       500\n",
            "          51       0.43      0.29      0.34       500\n",
            "          52       0.42      0.37      0.39       500\n",
            "          53       0.39      0.31      0.34       500\n",
            "          54       0.37      0.39      0.38       500\n",
            "          55       0.53      0.33      0.41       500\n",
            "          56       0.53      0.26      0.34       500\n",
            "          57       0.62      0.32      0.42       500\n",
            "          58       0.54      0.49      0.51       500\n",
            "          59       0.51      0.36      0.42       500\n",
            "          60       0.59      0.25      0.36       500\n",
            "          61       0.54      0.26      0.35       500\n",
            "          62       0.37      0.27      0.32       500\n",
            "          63       0.43      0.28      0.34       500\n",
            "          64       0.24      0.18      0.21       500\n",
            "          65       0.38      0.23      0.28       500\n",
            "          66       0.62      0.29      0.40       500\n",
            "          67       0.43      0.23      0.30       500\n",
            "          68       0.24      0.51      0.32       500\n",
            "          69       0.14      0.29      0.19       500\n",
            "          70       0.58      0.30      0.40       500\n",
            "          71       0.32      0.43      0.37       500\n",
            "          72       0.61      0.25      0.36       500\n",
            "          73       0.38      0.38      0.38       500\n",
            "          74       0.17      0.49      0.26       500\n",
            "          75       0.50      0.20      0.28       500\n",
            "          76       0.33      0.30      0.32       500\n",
            "          77       0.71      0.17      0.27       500\n",
            "          78       0.44      0.36      0.39       500\n",
            "          79       0.45      0.18      0.26       500\n",
            "          80       0.33      0.20      0.25       500\n",
            "          81       0.25      0.58      0.35       500\n",
            "          82       0.29      0.42      0.34       500\n",
            "          83       0.53      0.26      0.35       500\n",
            "          84       0.54      0.23      0.32       500\n",
            "          85       0.54      0.25      0.34       500\n",
            "          86       0.19      0.33      0.24       500\n",
            "          87       0.55      0.25      0.34       500\n",
            "          88       0.38      0.23      0.29       500\n",
            "          89       0.36      0.30      0.33       500\n",
            "          90       0.21      0.49      0.29       500\n",
            "          91       0.23      0.53      0.32       500\n",
            "          92       0.15      0.41      0.22       500\n",
            "          93       0.23      0.44      0.30       500\n",
            "          94       0.59      0.34      0.43       500\n",
            "          95       0.32      0.40      0.36       500\n",
            "          96       0.39      0.29      0.33       500\n",
            "          97       0.19      0.42      0.26       500\n",
            "          98       0.28      0.35      0.31       500\n",
            "          99       0.40      0.15      0.22       500\n",
            "         100       0.54      0.17      0.26       500\n",
            "         101       0.35      0.52      0.42       500\n",
            "         102       0.66      0.23      0.34       500\n",
            "         103       0.75      0.36      0.48       500\n",
            "         104       0.34      0.31      0.33       500\n",
            "         105       0.44      0.15      0.22       500\n",
            "         106       0.75      0.10      0.17       500\n",
            "         107       0.44      0.40      0.42       500\n",
            "         108       0.49      0.43      0.46       500\n",
            "         109       0.23      0.38      0.29       500\n",
            "         110       0.33      0.28      0.30       500\n",
            "         111       0.61      0.38      0.47       500\n",
            "         112       0.47      0.26      0.34       500\n",
            "         113       0.45      0.22      0.30       500\n",
            "         114       0.46      0.40      0.43       500\n",
            "         115       0.36      0.62      0.45       500\n",
            "         116       0.33      0.42      0.37       500\n",
            "         117       0.81      0.24      0.37       500\n",
            "         118       0.56      0.40      0.47       500\n",
            "         119       0.69      0.17      0.28       500\n",
            "         120       0.46      0.23      0.30       500\n",
            "         121       0.33      0.33      0.33       500\n",
            "         122       0.18      0.17      0.17       500\n",
            "         123       0.44      0.29      0.35       500\n",
            "         124       0.52      0.35      0.42       500\n",
            "         125       0.52      0.19      0.28       500\n",
            "         126       0.20      0.44      0.27       500\n",
            "         127       0.31      0.29      0.30       500\n",
            "         128       0.48      0.27      0.35       500\n",
            "         129       0.49      0.35      0.41       500\n",
            "         130       0.26      0.30      0.28       500\n",
            "         131       0.28      0.20      0.24       500\n",
            "         132       0.26      0.23      0.24       500\n",
            "         133       0.60      0.44      0.51       500\n",
            "         134       0.50      0.28      0.35       500\n",
            "         135       0.47      0.26      0.34       500\n",
            "         136       0.24      0.37      0.29       500\n",
            "         137       0.21      0.25      0.23       500\n",
            "         138       0.22      0.27      0.24       500\n",
            "         139       0.62      0.19      0.29       500\n",
            "         140       0.18      0.46      0.25       500\n",
            "         141       0.54      0.25      0.34       500\n",
            "         142       0.27      0.37      0.31       500\n",
            "         143       0.56      0.47      0.51       500\n",
            "         144       0.24      0.32      0.27       500\n",
            "         145       0.47      0.62      0.53       500\n",
            "         146       0.42      0.50      0.45       500\n",
            "         147       0.43      0.21      0.28       500\n",
            "         148       0.33      0.50      0.40       500\n",
            "         149       0.37      0.28      0.32       500\n",
            "         150       0.76      0.25      0.37       500\n",
            "         151       0.54      0.22      0.32       500\n",
            "         152       0.43      0.31      0.36       500\n",
            "         153       0.64      0.27      0.38       500\n",
            "         154       0.43      0.35      0.38       500\n",
            "         155       0.53      0.37      0.43       500\n",
            "         156       0.71      0.24      0.36       500\n",
            "         157       0.28      0.38      0.32       500\n",
            "         158       0.33      0.33      0.33       500\n",
            "         159       0.13      0.10      0.11       500\n",
            "         160       0.44      0.24      0.31       500\n",
            "         161       0.82      0.25      0.38       500\n",
            "         162       0.40      0.33      0.36       500\n",
            "         163       0.38      0.38      0.38       500\n",
            "         164       0.86      0.18      0.30       500\n",
            "         165       0.32      0.50      0.39       500\n",
            "         166       0.41      0.52      0.46       500\n",
            "         167       0.15      0.41      0.22       500\n",
            "         168       0.32      0.22      0.26       500\n",
            "         169       0.71      0.19      0.29       500\n",
            "         170       0.64      0.41      0.50       500\n",
            "         171       0.17      0.42      0.24       500\n",
            "         172       0.39      0.13      0.20       500\n",
            "         173       0.34      0.55      0.42       500\n",
            "         174       0.31      0.26      0.28       500\n",
            "         175       0.28      0.22      0.24       500\n",
            "         176       0.40      0.51      0.45       500\n",
            "         177       0.12      0.39      0.18       500\n",
            "         178       0.59      0.30      0.40       500\n",
            "         179       0.18      0.29      0.22       500\n",
            "         180       0.28      0.29      0.29       500\n",
            "         181       0.22      0.47      0.30       500\n",
            "         182       0.12      0.60      0.21       500\n",
            "         183       0.62      0.35      0.45       500\n",
            "         184       0.34      0.62      0.44       500\n",
            "         185       0.48      0.33      0.39       500\n",
            "         186       0.32      0.60      0.42       500\n",
            "         187       0.47      0.50      0.49       500\n",
            "         188       0.33      0.47      0.39       500\n",
            "         189       0.43      0.53      0.48       500\n",
            "         190       0.23      0.45      0.30       500\n",
            "         191       0.43      0.40      0.42       500\n",
            "         192       0.20      0.41      0.27       500\n",
            "         193       0.12      0.63      0.20       500\n",
            "         194       0.47      0.35      0.40       500\n",
            "         195       0.45      0.20      0.28       500\n",
            "         196       0.49      0.49      0.49       500\n",
            "         197       0.24      0.37      0.29       500\n",
            "         198       0.27      0.32      0.29       500\n",
            "         199       0.41      0.33      0.36       500\n",
            "\n",
            "   micro avg       0.34      0.34      0.34    100000\n",
            "   macro avg       0.42      0.34      0.35    100000\n",
            "weighted avg       0.42      0.34      0.35    100000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYD2NfKZuOTT",
        "colab_type": "code",
        "outputId": "ec3d6b24-23d2-44d7-fef1-c0a79e81e779",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3417
        }
      },
      "source": [
        "train_generator.class_indices"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'n01443537': 0,\n",
              " 'n01629819': 1,\n",
              " 'n01641577': 2,\n",
              " 'n01644900': 3,\n",
              " 'n01698640': 4,\n",
              " 'n01742172': 5,\n",
              " 'n01768244': 6,\n",
              " 'n01770393': 7,\n",
              " 'n01774384': 8,\n",
              " 'n01774750': 9,\n",
              " 'n01784675': 10,\n",
              " 'n01855672': 11,\n",
              " 'n01882714': 12,\n",
              " 'n01910747': 13,\n",
              " 'n01917289': 14,\n",
              " 'n01944390': 15,\n",
              " 'n01945685': 16,\n",
              " 'n01950731': 17,\n",
              " 'n01983481': 18,\n",
              " 'n01984695': 19,\n",
              " 'n02002724': 20,\n",
              " 'n02056570': 21,\n",
              " 'n02058221': 22,\n",
              " 'n02074367': 23,\n",
              " 'n02085620': 24,\n",
              " 'n02094433': 25,\n",
              " 'n02099601': 26,\n",
              " 'n02099712': 27,\n",
              " 'n02106662': 28,\n",
              " 'n02113799': 29,\n",
              " 'n02123045': 30,\n",
              " 'n02123394': 31,\n",
              " 'n02124075': 32,\n",
              " 'n02125311': 33,\n",
              " 'n02129165': 34,\n",
              " 'n02132136': 35,\n",
              " 'n02165456': 36,\n",
              " 'n02190166': 37,\n",
              " 'n02206856': 38,\n",
              " 'n02226429': 39,\n",
              " 'n02231487': 40,\n",
              " 'n02233338': 41,\n",
              " 'n02236044': 42,\n",
              " 'n02268443': 43,\n",
              " 'n02279972': 44,\n",
              " 'n02281406': 45,\n",
              " 'n02321529': 46,\n",
              " 'n02364673': 47,\n",
              " 'n02395406': 48,\n",
              " 'n02403003': 49,\n",
              " 'n02410509': 50,\n",
              " 'n02415577': 51,\n",
              " 'n02423022': 52,\n",
              " 'n02437312': 53,\n",
              " 'n02480495': 54,\n",
              " 'n02481823': 55,\n",
              " 'n02486410': 56,\n",
              " 'n02504458': 57,\n",
              " 'n02509815': 58,\n",
              " 'n02666196': 59,\n",
              " 'n02669723': 60,\n",
              " 'n02699494': 61,\n",
              " 'n02730930': 62,\n",
              " 'n02769748': 63,\n",
              " 'n02788148': 64,\n",
              " 'n02791270': 65,\n",
              " 'n02793495': 66,\n",
              " 'n02795169': 67,\n",
              " 'n02802426': 68,\n",
              " 'n02808440': 69,\n",
              " 'n02814533': 70,\n",
              " 'n02814860': 71,\n",
              " 'n02815834': 72,\n",
              " 'n02823428': 73,\n",
              " 'n02837789': 74,\n",
              " 'n02841315': 75,\n",
              " 'n02843684': 76,\n",
              " 'n02883205': 77,\n",
              " 'n02892201': 78,\n",
              " 'n02906734': 79,\n",
              " 'n02909870': 80,\n",
              " 'n02917067': 81,\n",
              " 'n02927161': 82,\n",
              " 'n02948072': 83,\n",
              " 'n02950826': 84,\n",
              " 'n02963159': 85,\n",
              " 'n02977058': 86,\n",
              " 'n02988304': 87,\n",
              " 'n02999410': 88,\n",
              " 'n03014705': 89,\n",
              " 'n03026506': 90,\n",
              " 'n03042490': 91,\n",
              " 'n03085013': 92,\n",
              " 'n03089624': 93,\n",
              " 'n03100240': 94,\n",
              " 'n03126707': 95,\n",
              " 'n03160309': 96,\n",
              " 'n03179701': 97,\n",
              " 'n03201208': 98,\n",
              " 'n03250847': 99,\n",
              " 'n03255030': 100,\n",
              " 'n03355925': 101,\n",
              " 'n03388043': 102,\n",
              " 'n03393912': 103,\n",
              " 'n03400231': 104,\n",
              " 'n03404251': 105,\n",
              " 'n03424325': 106,\n",
              " 'n03444034': 107,\n",
              " 'n03447447': 108,\n",
              " 'n03544143': 109,\n",
              " 'n03584254': 110,\n",
              " 'n03599486': 111,\n",
              " 'n03617480': 112,\n",
              " 'n03637318': 113,\n",
              " 'n03649909': 114,\n",
              " 'n03662601': 115,\n",
              " 'n03670208': 116,\n",
              " 'n03706229': 117,\n",
              " 'n03733131': 118,\n",
              " 'n03763968': 119,\n",
              " 'n03770439': 120,\n",
              " 'n03796401': 121,\n",
              " 'n03804744': 122,\n",
              " 'n03814639': 123,\n",
              " 'n03837869': 124,\n",
              " 'n03838899': 125,\n",
              " 'n03854065': 126,\n",
              " 'n03891332': 127,\n",
              " 'n03902125': 128,\n",
              " 'n03930313': 129,\n",
              " 'n03937543': 130,\n",
              " 'n03970156': 131,\n",
              " 'n03976657': 132,\n",
              " 'n03977966': 133,\n",
              " 'n03980874': 134,\n",
              " 'n03983396': 135,\n",
              " 'n03992509': 136,\n",
              " 'n04008634': 137,\n",
              " 'n04023962': 138,\n",
              " 'n04067472': 139,\n",
              " 'n04070727': 140,\n",
              " 'n04074963': 141,\n",
              " 'n04099969': 142,\n",
              " 'n04118538': 143,\n",
              " 'n04133789': 144,\n",
              " 'n04146614': 145,\n",
              " 'n04149813': 146,\n",
              " 'n04179913': 147,\n",
              " 'n04251144': 148,\n",
              " 'n04254777': 149,\n",
              " 'n04259630': 150,\n",
              " 'n04265275': 151,\n",
              " 'n04275548': 152,\n",
              " 'n04285008': 153,\n",
              " 'n04311004': 154,\n",
              " 'n04328186': 155,\n",
              " 'n04356056': 156,\n",
              " 'n04366367': 157,\n",
              " 'n04371430': 158,\n",
              " 'n04376876': 159,\n",
              " 'n04398044': 160,\n",
              " 'n04399382': 161,\n",
              " 'n04417672': 162,\n",
              " 'n04456115': 163,\n",
              " 'n04465501': 164,\n",
              " 'n04486054': 165,\n",
              " 'n04487081': 166,\n",
              " 'n04501370': 167,\n",
              " 'n04507155': 168,\n",
              " 'n04532106': 169,\n",
              " 'n04532670': 170,\n",
              " 'n04540053': 171,\n",
              " 'n04560804': 172,\n",
              " 'n04562935': 173,\n",
              " 'n04596742': 174,\n",
              " 'n04597913': 175,\n",
              " 'n06596364': 176,\n",
              " 'n07579787': 177,\n",
              " 'n07583066': 178,\n",
              " 'n07614500': 179,\n",
              " 'n07615774': 180,\n",
              " 'n07695742': 181,\n",
              " 'n07711569': 182,\n",
              " 'n07715103': 183,\n",
              " 'n07720875': 184,\n",
              " 'n07734744': 185,\n",
              " 'n07747607': 186,\n",
              " 'n07749582': 187,\n",
              " 'n07753592': 188,\n",
              " 'n07768694': 189,\n",
              " 'n07871810': 190,\n",
              " 'n07873807': 191,\n",
              " 'n07875152': 192,\n",
              " 'n07920052': 193,\n",
              " 'n09193705': 194,\n",
              " 'n09246464': 195,\n",
              " 'n09256479': 196,\n",
              " 'n09332890': 197,\n",
              " 'n09428293': 198,\n",
              " 'n12267677': 199}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tliiT2BkXazM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = pd.DataFrame(validation_generator.classes, y_pred, columns=['Predicted']).reset_index()\n",
        "results['True'] = results['index'] == results['Predicted']\n",
        "results['True']=results['True'].astype(int)\n",
        "label_map = (train_generator.class_indices)\n",
        "label_map = dict((v,k) for k,v in label_map.items()) #flip k,v\n",
        "results['predict_class'] = [label_map[k] for k in results['index']]\n",
        "results = results.groupby('predict_class')['True'].sum().to_dict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "984IBuB1cYOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_class_weights(dict_labels_vs_samples, balanced=True):\n",
        "  # param dict_labels_vs_samples: dict of 'label' vs. 'number of correct predictions'\n",
        "  # param balanced: if True, will produce class weights considering max samples count - % of augmented images per class will be proportional to max count of samples \n",
        "  # param balanced: if False, will produce class weights considering average samples count - % of aumented images per class will be proportional to avg samples of classes\n",
        "  \n",
        "  keys = dict_labels_vs_samples.keys()\n",
        "  values = list(dict_labels_vs_samples.values())\n",
        "  total_samples = sum(values)\n",
        "  num_classes = len(values)\n",
        "  max_of_all_classes = max(values)\n",
        "  average_of_all_classes = total_samples / num_classes\n",
        "  multiplying_factor = 1\n",
        "  \n",
        "  if balanced:\n",
        "    multiplying_factor = max_of_all_classes / average_of_all_classes\n",
        "  \n",
        "  print('total_samples: ', total_samples)\n",
        "  print('num_classes: ', num_classes)\n",
        "  print('max_of_all_classes: ', max_of_all_classes)\n",
        "  print('multiplying_factor: ', multiplying_factor)\n",
        "  \n",
        "  class_weight = dict()\n",
        "\n",
        "  for key in keys:\n",
        "        score_for_class = (total_samples / (num_classes * dict_labels_vs_samples.get(key))) * multiplying_factor\n",
        "        class_weight[key] = score_for_class\n",
        "        \n",
        "  return class_weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jzwM7_scmuu",
        "colab_type": "code",
        "outputId": "7ec580af-e8ff-43c1-e494-81687df18031",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "weights = get_class_weights(results, True)\n",
        "print(weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total_samples:  5714\n",
            "num_classes:  200\n",
            "max_of_all_classes:  45\n",
            "multiplying_factor:  1.575078753937697\n",
            "{'n01443537': 0.9999999999999999, 'n01629819': 1.1842105263157894, 'n01641577': 1.6071428571428574, 'n01644900': 2.25, 'n01698640': 1.6071428571428574, 'n01742172': 2.25, 'n01768244': 1.323529411764706, 'n01770393': 1.875, 'n01774384': 1.1538461538461537, 'n01774750': 1.6666666666666665, 'n01784675': 1.7307692307692308, 'n01855672': 1.5, 'n01882714': 1.25, 'n01910747': 1.1842105263157894, 'n01917289': 1.25, 'n01944390': 1.875, 'n01945685': 2.3684210526315788, 'n01950731': 1.25, 'n01983481': 1.6071428571428574, 'n01984695': 1.3636363636363635, 'n02002724': 1.323529411764706, 'n02056570': 1.3636363636363635, 'n02058221': 1.3636363636363635, 'n02074367': 0.9999999999999999, 'n02085620': 1.875, 'n02094433': 1.2162162162162162, 'n02099601': 1.5517241379310347, 'n02099712': 2.8125, 'n02106662': 1.8, 'n02113799': 2.647058823529412, 'n02123045': 1.7307692307692308, 'n02123394': 1.40625, 'n02124075': 2.3684210526315788, 'n02125311': 1.8, 'n02129165': 1.2162162162162162, 'n02132136': 1.323529411764706, 'n02165456': 1.125, 'n02190166': 1.5, 'n02206856': 1.3636363636363635, 'n02226429': 1.5517241379310347, 'n02231487': 2.25, 'n02233338': 1.956521739130435, 'n02236044': 2.142857142857143, 'n02268443': 1.8, 'n02279972': 1.0465116279069766, 'n02281406': 1.1538461538461537, 'n02321529': 1.4516129032258065, 'n02364673': 1.4516129032258065, 'n02395406': 2.8125, 'n02403003': 1.8, 'n02410509': 1.1842105263157894, 'n02415577': 1.5517241379310347, 'n02423022': 1.1842105263157894, 'n02437312': 1.1538461538461537, 'n02480495': 1.4516129032258065, 'n02481823': 1.323529411764706, 'n02486410': 1.8, 'n02504458': 1.2857142857142858, 'n02509815': 1.1538461538461537, 'n02666196': 1.956521739130435, 'n02669723': 1.25, 'n02699494': 1.5517241379310347, 'n02730930': 2.25, 'n02769748': 1.5517241379310347, 'n02788148': 3.214285714285715, 'n02791270': 2.0454545454545454, 'n02793495': 1.5, 'n02795169': 2.25, 'n02802426': 1.5517241379310347, 'n02808440': 1.8, 'n02814533': 1.6666666666666665, 'n02814860': 1.125, 'n02815834': 2.0454545454545454, 'n02823428': 1.6666666666666665, 'n02837789': 1.323529411764706, 'n02841315': 1.6666666666666665, 'n02843684': 1.7307692307692308, 'n02883205': 2.647058823529412, 'n02892201': 1.25, 'n02906734': 2.3684210526315788, 'n02909870': 3.4615384615384617, 'n02917067': 0.9999999999999999, 'n02927161': 1.4516129032258065, 'n02948072': 1.875, 'n02950826': 2.25, 'n02963159': 2.0454545454545454, 'n02977058': 1.6071428571428574, 'n02988304': 2.647058823529412, 'n02999410': 2.5, 'n03014705': 1.8, 'n03026506': 1.2857142857142858, 'n03042490': 1.40625, 'n03085013': 1.40625, 'n03089624': 1.3636363636363635, 'n03100240': 1.6666666666666665, 'n03126707': 1.6666666666666665, 'n03160309': 1.7307692307692308, 'n03179701': 1.5517241379310347, 'n03201208': 1.40625, 'n03250847': 2.647058823529412, 'n03255030': 3.75, 'n03355925': 1.40625, 'n03388043': 1.4516129032258065, 'n03393912': 1.0714285714285714, 'n03400231': 2.3684210526315788, 'n03404251': 2.5, 'n03424325': 2.0454545454545454, 'n03444034': 1.1538461538461537, 'n03447447': 1.0465116279069766, 'n03544143': 1.5, 'n03584254': 1.6071428571428574, 'n03599486': 1.323529411764706, 'n03617480': 2.3684210526315788, 'n03637318': 1.6666666666666665, 'n03649909': 2.142857142857143, 'n03662601': 1.0714285714285714, 'n03670208': 1.5517241379310347, 'n03706229': 1.5, 'n03733131': 1.1538461538461537, 'n03763968': 2.142857142857143, 'n03770439': 2.0454545454545454, 'n03796401': 1.4516129032258065, 'n03804744': 2.647058823529412, 'n03814639': 2.0454545454545454, 'n03837869': 1.097560975609756, 'n03838899': 2.647058823529412, 'n03854065': 1.40625, 'n03891332': 1.5517241379310347, 'n03902125': 1.5517241379310347, 'n03930313': 1.2162162162162162, 'n03937543': 2.142857142857143, 'n03970156': 6.42857142857143, 'n03976657': 3.75, 'n03977966': 1.1538461538461537, 'n03980874': 1.875, 'n03983396': 3.4615384615384617, 'n03992509': 1.5, 'n04008634': 2.8125, 'n04023962': 2.647058823529412, 'n04067472': 2.647058823529412, 'n04070727': 1.4516129032258065, 'n04074963': 1.956521739130435, 'n04099969': 1.875, 'n04118538': 1.0714285714285714, 'n04133789': 1.6666666666666665, 'n04146614': 0.9999999999999999, 'n04149813': 1.2857142857142858, 'n04179913': 1.7307692307692308, 'n04251144': 1.875, 'n04254777': 1.2857142857142858, 'n04259630': 1.6666666666666665, 'n04265275': 1.6071428571428574, 'n04275548': 1.323529411764706, 'n04285008': 1.5, 'n04311004': 1.8, 'n04328186': 1.5, 'n04356056': 1.6666666666666665, 'n04366367': 1.6071428571428574, 'n04371430': 2.3684210526315788, 'n04376876': 3.4615384615384617, 'n04398044': 2.0454545454545454, 'n04399382': 1.3636363636363635, 'n04417672': 1.40625, 'n04456115': 1.4516129032258065, 'n04465501': 1.5517241379310347, 'n04486054': 1.125, 'n04487081': 1.0714285714285714, 'n04501370': 1.6666666666666665, 'n04507155': 3.4615384615384617, 'n04532106': 1.4516129032258065, 'n04532670': 1.097560975609756, 'n04540053': 1.1538461538461537, 'n04560804': 2.5, 'n04562935': 1.125, 'n04596742': 1.5517241379310347, 'n04597913': 4.090909090909091, 'n06596364': 1.1538461538461537, 'n07579787': 1.8, 'n07583066': 1.323529411764706, 'n07614500': 2.25, 'n07615774': 2.25, 'n07695742': 1.6666666666666665, 'n07711569': 1.7307692307692308, 'n07715103': 1.6071428571428574, 'n07720875': 1.2162162162162162, 'n07734744': 1.25, 'n07747607': 1.40625, 'n07749582': 1.3636363636363635, 'n07753592': 1.4516129032258065, 'n07768694': 1.125, 'n07871810': 1.6666666666666665, 'n07873807': 1.2162162162162162, 'n07875152': 1.5, 'n07920052': 1.0714285714285714, 'n09193705': 1.323529411764706, 'n09246464': 1.5, 'n09256479': 1.2162162162162162, 'n09332890': 2.25, 'n09428293': 1.6071428571428574, 'n12267677': 1.8}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JroHWbtLlBQP",
        "colab_type": "code",
        "outputId": "89e4ac1e-e9d1-47e7-def2-d1d4b2d18677",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory( r'./tinyimagenet/train', target_size=(64, 64), color_mode='rgb', \n",
        "                                                    batch_size=batch_size, class_mode='categorical', seed=42, classes=weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxxiJuvXlKjT",
        "colab_type": "code",
        "outputId": "75fa49a9-287b-411b-9dca-c6d09d989572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "validation_generator = valid_datagen.flow_from_dataframe(val_data, directory='./tinyimagenet/val/images/', x_col='File', y_col='Class', target_size=(64, 64),\n",
        "                                                    color_mode='rgb', class_mode='categorical', batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10000 images belonging to 200 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKVpajqTlgIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    if epoch > 100:\n",
        "        lr = 0.5e-5\n",
        "    elif epoch > 75:\n",
        "        lr = 1e-5\n",
        "    elif epoch > 50:\n",
        "        lr = 0.5e-4\n",
        "    elif epoch > 30:\n",
        "        lr = 1e-4\n",
        "    else:\n",
        "        lr = 1e-3\n",
        "    print('Learning rate (from LearningRateScheduler): ', lr)\n",
        "    return lr\n",
        "\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0vc1Ofxlpbp",
        "colab_type": "code",
        "outputId": "48081e3e-b78e-4874-8dd8-a9502748aa62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8160
        }
      },
      "source": [
        "tpu_model.fit_generator(train_generator, \n",
        "                        epochs=125, \n",
        "                        steps_per_epoch=int(100000//batch_size),\n",
        "                        validation_steps=int(10000//batch_size), \n",
        "                        validation_data=validation_generator,\n",
        "                        callbacks=[checkpoint, lr_scheduler, csv_logger, lr_reducer] )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 1/125\n",
            "5/5 [==============================] - 15s 3s/step - loss: 3.8297 - acc: 0.3621\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 215s 4s/step - loss: 3.6923 - acc: 0.2731 - val_loss: 3.8297 - val_acc: 0.3621\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 2/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 5.0913 - acc: 0.2538\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 208s 4s/step - loss: 3.5333 - acc: 0.3023 - val_loss: 5.0913 - val_acc: 0.2538\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 3/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 3.1596 - acc: 0.4047\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 209s 4s/step - loss: 3.5447 - acc: 0.3021 - val_loss: 3.1596 - val_acc: 0.4047\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 4/125\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.8676 - acc: 0.4548\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 214s 4s/step - loss: 3.4994 - acc: 0.3112 - val_loss: 2.8676 - val_acc: 0.4548\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 5/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 3.2077 - acc: 0.4196\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 211s 4s/step - loss: 3.4510 - acc: 0.3230 - val_loss: 3.2077 - val_acc: 0.4196\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 6/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.8276 - acc: 0.4485\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 212s 4s/step - loss: 3.4277 - acc: 0.3264 - val_loss: 2.8276 - val_acc: 0.4485\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 7/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.9927 - acc: 0.4528\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 214s 4s/step - loss: 3.4022 - acc: 0.3306 - val_loss: 2.9927 - val_acc: 0.4528\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 8/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.8703 - acc: 0.4455\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 209s 4s/step - loss: 3.3899 - acc: 0.3349 - val_loss: 2.8703 - val_acc: 0.4455\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 9/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.8346 - acc: 0.4547\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 209s 4s/step - loss: 3.3849 - acc: 0.3355 - val_loss: 2.8346 - val_acc: 0.4547\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 10/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 3.0464 - acc: 0.4336\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 211s 4s/step - loss: 3.3583 - acc: 0.3406 - val_loss: 3.0464 - val_acc: 0.4336\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 11/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.8085 - acc: 0.4760\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 208s 4s/step - loss: 3.3618 - acc: 0.3409 - val_loss: 2.8085 - val_acc: 0.4760\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 12/125\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.8330 - acc: 0.4594\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 212s 4s/step - loss: 3.3357 - acc: 0.3450 - val_loss: 2.8330 - val_acc: 0.4594\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 13/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 3.2046 - acc: 0.4345\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 212s 4s/step - loss: 3.3097 - acc: 0.3530 - val_loss: 3.2046 - val_acc: 0.4345\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 14/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 3.0252 - acc: 0.4476\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 209s 4s/step - loss: 3.3324 - acc: 0.3484 - val_loss: 3.0252 - val_acc: 0.4476\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 15/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.9395 - acc: 0.4666\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 211s 4s/step - loss: 3.3025 - acc: 0.3513 - val_loss: 2.9395 - val_acc: 0.4666\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 16/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.8283 - acc: 0.4677\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 210s 4s/step - loss: 3.2896 - acc: 0.3545 - val_loss: 2.8283 - val_acc: 0.4677\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 17/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.6989 - acc: 0.4747\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 210s 4s/step - loss: 3.2871 - acc: 0.3562 - val_loss: 2.6989 - val_acc: 0.4747\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 18/125\n",
            "5/5 [==============================] - 14s 3s/step - loss: 3.0504 - acc: 0.4471\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 214s 4s/step - loss: 3.2721 - acc: 0.3574 - val_loss: 3.0504 - val_acc: 0.4471\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 19/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.8974 - acc: 0.4707\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 211s 4s/step - loss: 3.2807 - acc: 0.3578 - val_loss: 2.8974 - val_acc: 0.4707\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 20/125\n",
            "5/5 [==============================] - 15s 3s/step - loss: 2.7780 - acc: 0.4730\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 212s 4s/step - loss: 3.2718 - acc: 0.3605 - val_loss: 2.7780 - val_acc: 0.4730\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 21/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 3.0357 - acc: 0.4571\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.57350\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "49/49 [==============================] - 210s 4s/step - loss: 3.2635 - acc: 0.3609 - val_loss: 3.0357 - val_acc: 0.4571\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 22/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 3.3270 - acc: 0.4290\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 214s 4s/step - loss: 3.2322 - acc: 0.3684 - val_loss: 3.3270 - val_acc: 0.4290\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 23/125\n",
            "5/5 [==============================] - 14s 3s/step - loss: 3.0983 - acc: 0.4513\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 213s 4s/step - loss: 3.2481 - acc: 0.3634 - val_loss: 3.0983 - val_acc: 0.4513\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 24/125\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.8528 - acc: 0.4778\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 212s 4s/step - loss: 3.2475 - acc: 0.3644 - val_loss: 2.8528 - val_acc: 0.4778\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 25/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.9079 - acc: 0.4768\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 212s 4s/step - loss: 3.2257 - acc: 0.3705 - val_loss: 2.9079 - val_acc: 0.4768\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 26/125\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.9317 - acc: 0.4531\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 211s 4s/step - loss: 3.2498 - acc: 0.3653 - val_loss: 2.9317 - val_acc: 0.4531\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 27/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.7719 - acc: 0.4746\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 208s 4s/step - loss: 3.2324 - acc: 0.3664 - val_loss: 2.7719 - val_acc: 0.4746\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 28/125\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.9089 - acc: 0.4678\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 211s 4s/step - loss: 3.2228 - acc: 0.3686 - val_loss: 2.9089 - val_acc: 0.4678\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 29/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.7121 - acc: 0.4901\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 210s 4s/step - loss: 3.2253 - acc: 0.3699 - val_loss: 2.7121 - val_acc: 0.4901\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 30/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.9385 - acc: 0.4796\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 210s 4s/step - loss: 3.2169 - acc: 0.3714 - val_loss: 2.9385 - val_acc: 0.4796\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 31/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 3.1977 - acc: 0.4602\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 211s 4s/step - loss: 3.2163 - acc: 0.3714 - val_loss: 3.1977 - val_acc: 0.4602\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 32/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.2988 - acc: 0.5625\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 209s 4s/step - loss: 3.0494 - acc: 0.4076 - val_loss: 2.2988 - val_acc: 0.5625\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 33/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.2514 - acc: 0.5690\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.57350\n",
            "49/49 [==============================] - 209s 4s/step - loss: 2.9757 - acc: 0.4242 - val_loss: 2.2514 - val_acc: 0.5690\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 34/125\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.2195 - acc: 0.5747\n",
            "\n",
            "Epoch 00034: val_acc improved from 0.57350 to 0.57470, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:034-val_acc:0.575.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 9.999999747378752e-05\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 249s 5s/step - loss: 2.9582 - acc: 0.4266 - val_loss: 2.2195 - val_acc: 0.5747\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 35/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1942 - acc: 0.5782\n",
            "\n",
            "Epoch 00035: val_acc improved from 0.57470 to 0.57820, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:035-val_acc:0.578.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 9.999999747378752e-05\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 215s 4s/step - loss: 2.9278 - acc: 0.4331 - val_loss: 2.1942 - val_acc: 0.5782\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 36/125\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.2035 - acc: 0.5750\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.57820\n",
            "49/49 [==============================] - 173s 4s/step - loss: 2.9056 - acc: 0.4366 - val_loss: 2.2035 - val_acc: 0.5750\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 37/125\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.2008 - acc: 0.5771\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.57820\n",
            "49/49 [==============================] - 210s 4s/step - loss: 2.8820 - acc: 0.4398 - val_loss: 2.2008 - val_acc: 0.5771\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 38/125\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.1753 - acc: 0.5823\n",
            "\n",
            "Epoch 00038: val_acc improved from 0.57820 to 0.58230, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:038-val_acc:0.582.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 9.999999747378752e-05\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 251s 5s/step - loss: 2.8604 - acc: 0.4446 - val_loss: 2.1753 - val_acc: 0.5823\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 39/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1937 - acc: 0.5807\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.58230\n",
            "49/49 [==============================] - 176s 4s/step - loss: 2.8580 - acc: 0.4422 - val_loss: 2.1937 - val_acc: 0.5807\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 40/125\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.1757 - acc: 0.5832\n",
            "\n",
            "Epoch 00040: val_acc improved from 0.58230 to 0.58320, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:040-val_acc:0.583.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 9.999999747378752e-05\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 253s 5s/step - loss: 2.8437 - acc: 0.4449 - val_loss: 2.1757 - val_acc: 0.5832\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 41/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1716 - acc: 0.5793\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.58320\n",
            "49/49 [==============================] - 174s 4s/step - loss: 2.8286 - acc: 0.4477 - val_loss: 2.1716 - val_acc: 0.5793\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 42/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1594 - acc: 0.5797\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.58320\n",
            "49/49 [==============================] - 210s 4s/step - loss: 2.8182 - acc: 0.4494 - val_loss: 2.1594 - val_acc: 0.5797\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 43/125\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.1637 - acc: 0.5821\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.58320\n",
            "49/49 [==============================] - 211s 4s/step - loss: 2.8124 - acc: 0.4489 - val_loss: 2.1637 - val_acc: 0.5821\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 44/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1493 - acc: 0.5828\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.58320\n",
            "49/49 [==============================] - 212s 4s/step - loss: 2.8105 - acc: 0.4512 - val_loss: 2.1493 - val_acc: 0.5828\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 45/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1610 - acc: 0.5804\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.58320\n",
            "49/49 [==============================] - 209s 4s/step - loss: 2.7938 - acc: 0.4523 - val_loss: 2.1610 - val_acc: 0.5804\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 46/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1485 - acc: 0.5854\n",
            "\n",
            "Epoch 00046: val_acc improved from 0.58320 to 0.58540, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:046-val_acc:0.585.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 9.999999747378752e-05\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 253s 5s/step - loss: 2.7718 - acc: 0.4558 - val_loss: 2.1485 - val_acc: 0.5854\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 47/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1414 - acc: 0.5843\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.58540\n",
            "49/49 [==============================] - 174s 4s/step - loss: 2.7679 - acc: 0.4552 - val_loss: 2.1414 - val_acc: 0.5843\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 48/125\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.1307 - acc: 0.5844\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.58540\n",
            "49/49 [==============================] - 211s 4s/step - loss: 2.7540 - acc: 0.4599 - val_loss: 2.1307 - val_acc: 0.5844\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 49/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1576 - acc: 0.5762\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.58540\n",
            "49/49 [==============================] - 211s 4s/step - loss: 2.7543 - acc: 0.4585 - val_loss: 2.1576 - val_acc: 0.5762\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 50/125\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.1273 - acc: 0.5828\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.58540\n",
            "49/49 [==============================] - 214s 4s/step - loss: 2.7518 - acc: 0.4595 - val_loss: 2.1273 - val_acc: 0.5828\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 51/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1679 - acc: 0.5796\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.58540\n",
            "49/49 [==============================] - 210s 4s/step - loss: 2.7295 - acc: 0.4617 - val_loss: 2.1679 - val_acc: 0.5796\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 52/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1277 - acc: 0.5847\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.58540\n",
            "49/49 [==============================] - 211s 4s/step - loss: 2.7176 - acc: 0.4646 - val_loss: 2.1277 - val_acc: 0.5847\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 53/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1110 - acc: 0.5862\n",
            "\n",
            "Epoch 00053: val_acc improved from 0.58540 to 0.58620, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:053-val_acc:0.586.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 4.999999873689376e-05\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 249s 5s/step - loss: 2.7134 - acc: 0.4629 - val_loss: 2.1110 - val_acc: 0.5862\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 54/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1212 - acc: 0.5869\n",
            "\n",
            "Epoch 00054: val_acc improved from 0.58620 to 0.58690, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:054-val_acc:0.587.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 4.999999873689376e-05\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 214s 4s/step - loss: 2.7092 - acc: 0.4675 - val_loss: 2.1212 - val_acc: 0.5869\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 55/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1179 - acc: 0.5875\n",
            "\n",
            "Epoch 00055: val_acc improved from 0.58690 to 0.58750, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:055-val_acc:0.587.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 4.999999873689376e-05\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 215s 4s/step - loss: 2.6982 - acc: 0.4681 - val_loss: 2.1179 - val_acc: 0.5875\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 56/125\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.1072 - acc: 0.5888\n",
            "\n",
            "Epoch 00056: val_acc improved from 0.58750 to 0.58880, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:056-val_acc:0.589.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 4.999999873689376e-05\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 216s 4s/step - loss: 2.6943 - acc: 0.4678 - val_loss: 2.1072 - val_acc: 0.5888\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 57/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1115 - acc: 0.5884\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.58880\n",
            "49/49 [==============================] - 173s 4s/step - loss: 2.6924 - acc: 0.4681 - val_loss: 2.1115 - val_acc: 0.5884\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 58/125\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.1149 - acc: 0.5895\n",
            "\n",
            "Epoch 00058: val_acc improved from 0.58880 to 0.58950, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:058-val_acc:0.590.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 4.999999873689376e-05\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 253s 5s/step - loss: 2.6902 - acc: 0.4678 - val_loss: 2.1149 - val_acc: 0.5895\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 59/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1044 - acc: 0.5882\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.58950\n",
            "49/49 [==============================] - 175s 4s/step - loss: 2.6925 - acc: 0.4695 - val_loss: 2.1044 - val_acc: 0.5882\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 60/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1131 - acc: 0.5872\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.58950\n",
            "49/49 [==============================] - 213s 4s/step - loss: 2.6852 - acc: 0.4693 - val_loss: 2.1131 - val_acc: 0.5872\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 61/125\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.1119 - acc: 0.5864\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.58950\n",
            "49/49 [==============================] - 212s 4s/step - loss: 2.6699 - acc: 0.4722 - val_loss: 2.1119 - val_acc: 0.5864\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 62/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1072 - acc: 0.5863\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.58950\n",
            "49/49 [==============================] - 213s 4s/step - loss: 2.6765 - acc: 0.4702 - val_loss: 2.1072 - val_acc: 0.5863\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 63/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1261 - acc: 0.5871\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.58950\n",
            "49/49 [==============================] - 214s 4s/step - loss: 2.6736 - acc: 0.4723 - val_loss: 2.1261 - val_acc: 0.5871\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 64/125\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1185 - acc: 0.5848\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.58950\n",
            "49/49 [==============================] - 213s 4s/step - loss: 2.6674 - acc: 0.4717 - val_loss: 2.1185 - val_acc: 0.5848\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 65/125\n",
            "46/49 [===========================>..] - ETA: 12s - loss: 2.6555 - acc: 0.4739"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtHMUPiDLf9k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tpu_model.load_weights('/content/gdrive/My Drive/epochs_TPUCustomImageAugFinal:058-val_acc:0.590.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7jd8c0twRH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights = {'n01443537': 0.9999999999999999, 'n01629819': 1.1842105263157894, 'n01641577': 1.6071428571428574, 'n01644900': 2.25, 'n01698640': 1.6071428571428574, 'n01742172': 2.25, 'n01768244': 1.323529411764706, 'n01770393': 1.875, 'n01774384': 1.1538461538461537, 'n01774750': 1.6666666666666665, 'n01784675': 1.7307692307692308, 'n01855672': 1.5, 'n01882714': 1.25, 'n01910747': 1.1842105263157894, 'n01917289': 1.25, 'n01944390': 1.875, 'n01945685': 2.3684210526315788, 'n01950731': 1.25, 'n01983481': 1.6071428571428574, 'n01984695': 1.3636363636363635, 'n02002724': 1.323529411764706, 'n02056570': 1.3636363636363635, 'n02058221': 1.3636363636363635, 'n02074367': 0.9999999999999999, 'n02085620': 1.875, 'n02094433': 1.2162162162162162, 'n02099601': 1.5517241379310347, 'n02099712': 2.8125, 'n02106662': 1.8, 'n02113799': 2.647058823529412, 'n02123045': 1.7307692307692308, 'n02123394': 1.40625, 'n02124075': 2.3684210526315788, 'n02125311': 1.8, 'n02129165': 1.2162162162162162, 'n02132136': 1.323529411764706, 'n02165456': 1.125, 'n02190166': 1.5, 'n02206856': 1.3636363636363635, 'n02226429': 1.5517241379310347, 'n02231487': 2.25, 'n02233338': 1.956521739130435, 'n02236044': 2.142857142857143, 'n02268443': 1.8, 'n02279972': 1.0465116279069766, 'n02281406': 1.1538461538461537, 'n02321529': 1.4516129032258065, 'n02364673': 1.4516129032258065, 'n02395406': 2.8125, 'n02403003': 1.8, 'n02410509': 1.1842105263157894, 'n02415577': 1.5517241379310347, 'n02423022': 1.1842105263157894, 'n02437312': 1.1538461538461537, 'n02480495': 1.4516129032258065, 'n02481823': 1.323529411764706, 'n02486410': 1.8, 'n02504458': 1.2857142857142858, 'n02509815': 1.1538461538461537, 'n02666196': 1.956521739130435, 'n02669723': 1.25, 'n02699494': 1.5517241379310347, 'n02730930': 2.25, 'n02769748': 1.5517241379310347, 'n02788148': 3.214285714285715, 'n02791270': 2.0454545454545454, 'n02793495': 1.5, 'n02795169': 2.25, 'n02802426': 1.5517241379310347, 'n02808440': 1.8, 'n02814533': 1.6666666666666665, 'n02814860': 1.125, 'n02815834': 2.0454545454545454, 'n02823428': 1.6666666666666665, 'n02837789': 1.323529411764706, 'n02841315': 1.6666666666666665, 'n02843684': 1.7307692307692308, 'n02883205': 2.647058823529412, 'n02892201': 1.25, 'n02906734': 2.3684210526315788, 'n02909870': 3.4615384615384617, 'n02917067': 0.9999999999999999, 'n02927161': 1.4516129032258065, 'n02948072': 1.875, 'n02950826': 2.25, 'n02963159': 2.0454545454545454, 'n02977058': 1.6071428571428574, 'n02988304': 2.647058823529412, 'n02999410': 2.5, 'n03014705': 1.8, 'n03026506': 1.2857142857142858, 'n03042490': 1.40625, 'n03085013': 1.40625, 'n03089624': 1.3636363636363635, 'n03100240': 1.6666666666666665, 'n03126707': 1.6666666666666665, 'n03160309': 1.7307692307692308, 'n03179701': 1.5517241379310347, 'n03201208': 1.40625, 'n03250847': 2.647058823529412, 'n03255030': 3.75, 'n03355925': 1.40625, 'n03388043': 1.4516129032258065, 'n03393912': 1.0714285714285714, 'n03400231': 2.3684210526315788, 'n03404251': 2.5, 'n03424325': 2.0454545454545454, 'n03444034': 1.1538461538461537, 'n03447447': 1.0465116279069766, 'n03544143': 1.5, 'n03584254': 1.6071428571428574, 'n03599486': 1.323529411764706, 'n03617480': 2.3684210526315788, 'n03637318': 1.6666666666666665, 'n03649909': 2.142857142857143, 'n03662601': 1.0714285714285714, 'n03670208': 1.5517241379310347, 'n03706229': 1.5, 'n03733131': 1.1538461538461537, 'n03763968': 2.142857142857143, 'n03770439': 2.0454545454545454, 'n03796401': 1.4516129032258065, 'n03804744': 2.647058823529412, 'n03814639': 2.0454545454545454, 'n03837869': 1.097560975609756, 'n03838899': 2.647058823529412, 'n03854065': 1.40625, 'n03891332': 1.5517241379310347, 'n03902125': 1.5517241379310347, 'n03930313': 1.2162162162162162, 'n03937543': 2.142857142857143, 'n03970156': 6.42857142857143, 'n03976657': 3.75, 'n03977966': 1.1538461538461537, 'n03980874': 1.875, 'n03983396': 3.4615384615384617, 'n03992509': 1.5, 'n04008634': 2.8125, 'n04023962': 2.647058823529412, 'n04067472': 2.647058823529412, 'n04070727': 1.4516129032258065, 'n04074963': 1.956521739130435, 'n04099969': 1.875, 'n04118538': 1.0714285714285714, 'n04133789': 1.6666666666666665, 'n04146614': 0.9999999999999999, 'n04149813': 1.2857142857142858, 'n04179913': 1.7307692307692308, 'n04251144': 1.875, 'n04254777': 1.2857142857142858, 'n04259630': 1.6666666666666665, 'n04265275': 1.6071428571428574, 'n04275548': 1.323529411764706, 'n04285008': 1.5, 'n04311004': 1.8, 'n04328186': 1.5, 'n04356056': 1.6666666666666665, 'n04366367': 1.6071428571428574, 'n04371430': 2.3684210526315788, 'n04376876': 3.4615384615384617, 'n04398044': 2.0454545454545454, 'n04399382': 1.3636363636363635, 'n04417672': 1.40625, 'n04456115': 1.4516129032258065, 'n04465501': 1.5517241379310347, 'n04486054': 1.125, 'n04487081': 1.0714285714285714, 'n04501370': 1.6666666666666665, 'n04507155': 3.4615384615384617, 'n04532106': 1.4516129032258065, 'n04532670': 1.097560975609756, 'n04540053': 1.1538461538461537, 'n04560804': 2.5, 'n04562935': 1.125, 'n04596742': 1.5517241379310347, 'n04597913': 4.090909090909091, 'n06596364': 1.1538461538461537, 'n07579787': 1.8, 'n07583066': 1.323529411764706, 'n07614500': 2.25, 'n07615774': 2.25, 'n07695742': 1.6666666666666665, 'n07711569': 1.7307692307692308, 'n07715103': 1.6071428571428574, 'n07720875': 1.2162162162162162, 'n07734744': 1.25, 'n07747607': 1.40625, 'n07749582': 1.3636363636363635, 'n07753592': 1.4516129032258065, 'n07768694': 1.125, 'n07871810': 1.6666666666666665, 'n07873807': 1.2162162162162162, 'n07875152': 1.5, 'n07920052': 1.0714285714285714, 'n09193705': 1.323529411764706, 'n09246464': 1.5, 'n09256479': 1.2162162162162162, 'n09332890': 2.25, 'n09428293': 1.6071428571428574, 'n12267677': 1.8}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6e71EcNxZYj",
        "colab_type": "code",
        "outputId": "ae059334-af0d-47d5-a9e5-13db66a2f562",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory( r'./tinyimagenet/train', target_size=(64, 64), color_mode='rgb', \n",
        "                                                    batch_size=batch_size, class_mode='categorical', seed=42, classes=weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc9Sne2aKOZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):    \n",
        "    if epoch > 36:\n",
        "        lr = 0.5e-5\n",
        "    elif epoch > 11:\n",
        "        lr = 1e-5\n",
        "    else:\n",
        "        lr = 0.5e-4\n",
        "    print('Learning rate (from LearningRateScheduler): ', lr)\n",
        "    return lr\n",
        "\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC3zVSbRL4Q4",
        "colab_type": "code",
        "outputId": "d735672b-7d9c-48fe-aaf7-97f2c0bd729e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8027
        }
      },
      "source": [
        "tpu_model.fit_generator(train_generator, \n",
        "                        epochs=61, \n",
        "                        steps_per_epoch=int(100000//batch_size),\n",
        "                        validation_steps=int(10000//batch_size), \n",
        "                        validation_data=validation_generator,\n",
        "                        callbacks=[checkpoint, lr_scheduler, csv_logger, lr_reducer] )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 1/61\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(256,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(256, 64, 64, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(256, 200), dtype=tf.float32, name='activation_11_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 4.999999873689376e-05, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py:302: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f9548915b38> []\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 25.99946880340576 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "INFO:tensorflow:CPU -> TPU lr: 4.999999873689376e-05 {5e-05}\n",
            "INFO:tensorflow:CPU -> TPU beta_1: 0.8999999761581421 {0.9}\n",
            "INFO:tensorflow:CPU -> TPU beta_2: 0.9990000128746033 {0.999}\n",
            "INFO:tensorflow:CPU -> TPU decay: 0.0 {0.0}\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "29/49 [================>.............] - ETA: 1:44 - loss: 2.6877 - acc: 0.4697INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(212,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(212, 64, 64, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(212, 200), dtype=tf.float32, name='activation_11_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f9548915b38> [<tf.Variable 'tpu_140279153090800/Adam/iterations:0' shape=() dtype=int64>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9541340b00>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9541340ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f95412d9128>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9541297550>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f954124a7f0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9541226c50>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f95411ece10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f95411b1a20>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9541131e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f95410e57b8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9541107c88>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9541072ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540fd9668>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540ffbcf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540fa4a90>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540ed7828>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540efaba8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540ec1400>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9545811cc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540e87cc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540d23f60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540d23ba8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540d49320>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540d10908>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540cb0320>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540c1c748>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540be4828>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540c07dd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540bcdc88>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540add470>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540aa4320>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540a68518>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f95409f68d0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540a2de48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f95409bde10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540912b38>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f95408b7668>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f95408b7c50>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f954085c860>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540846d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f95407d3390>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f954080ce48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540742a20>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f95406cdb38>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f95406717f0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f95405daf28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f954059f7b8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540568d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f95405527f0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f95404f44a8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f95404bbba8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540447b70>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f954040bd30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f95403ee2e8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f954033db38>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f95403044e0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f95402c1e80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540291dd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540235eb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f95401fef28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540168f60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f954012ec50>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f9540096080>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f95400bc6a0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953ffef630>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953ffefdd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953ff59e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953ffb3518>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953ff44c50>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953fee56d8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953fe72fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953fe183c8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953fe3ed30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953fdffcc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953fdcb2b0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953fcfbfd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953fca0a20>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953fc8bcc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953fc691d0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953fbbde80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953fb81ba8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953fb27fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953fb0c588>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953fb47128>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953fa77a58>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f9feda0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f9c4240>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f969278>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f98b6d8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f89cd68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f865400>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f84bbe0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f75ceb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f722f28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f7226d8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f77d9b0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f70ce80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f6d41d0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f5dceb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f5dc710>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f5c9c18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f4d79b0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f4a1b70>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f4c45f8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f450b00>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f3baf60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f35da20>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f346748>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f346e80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f2d3eb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f27a6a0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f23f400>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f170320>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f170a58>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f0d8c18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f0a0860>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f069a58>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953f04c400>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953ef988d0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953efbed68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953ef2b5f8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953eef0358>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953eeb5668>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953ee7b7f0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953ede8be0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953ee08b00>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953ed74470>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953ecdcd30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953eca7438>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953ec90f98>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953ec45cc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953ebfab70>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953eb5bf60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953eb0a8d0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953ea5fda0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953eab69e8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953ea6e7f0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953ea76ac8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953e9b3fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953e9777b8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953e906278>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953e8e3e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953e870550>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953e8385f8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953e7c5940>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953e78afd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953e753dd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953e6f8ba8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953e684d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953e6b6da0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953e5eeef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953e558b70>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953e543828>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953e509d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953e4ae7b8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953e418550>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953e43bd30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953e401d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953e38e908>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953e2d8c18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953e2f9438>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953e287e80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953e264ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953e1b5ac8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f953e1b5ba8>]\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 29.171329975128174 secs\n",
            "48/49 [============================>.] - ETA: 5s - loss: 2.6902 - acc: 0.4687 INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(256,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(256, 64, 64, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(256, 200), dtype=tf.float32, name='activation_11_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 4.999999873689376e-05, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f95376d4550> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 15.823506832122803 secs\n",
            "4/5 [=======================>......] - ETA: 6s - loss: 2.0924 - acc: 0.5891 INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(226,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(226, 64, 64, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(226, 200), dtype=tf.float32, name='activation_11_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f95376d4550> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 11.22713017463684 secs\n",
            "5/5 [==============================] - 40s 8s/step - loss: 2.1068 - acc: 0.5875\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.58750, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinalv2:001-val_acc:0.587.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 4.999999873689376e-05\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 350s 7s/step - loss: 2.6920 - acc: 0.4684 - val_loss: 2.1068 - val_acc: 0.5875\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 2/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1165 - acc: 0.5868\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.58750\n",
            "49/49 [==============================] - 167s 3s/step - loss: 2.6845 - acc: 0.4698 - val_loss: 2.1165 - val_acc: 0.5868\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 3/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1207 - acc: 0.5906\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.58750 to 0.59060, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinalv2:003-val_acc:0.591.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 4.999999873689376e-05\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 232s 5s/step - loss: 2.6789 - acc: 0.4710 - val_loss: 2.1207 - val_acc: 0.5906\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 4/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1309 - acc: 0.5882\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.59060\n",
            "49/49 [==============================] - 178s 4s/step - loss: 2.6737 - acc: 0.4708 - val_loss: 2.1309 - val_acc: 0.5882\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 5/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1186 - acc: 0.5870\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.59060\n",
            "49/49 [==============================] - 202s 4s/step - loss: 2.6646 - acc: 0.4723 - val_loss: 2.1186 - val_acc: 0.5870\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 6/61\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.1217 - acc: 0.5881\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.59060\n",
            "49/49 [==============================] - 202s 4s/step - loss: 2.6652 - acc: 0.4731 - val_loss: 2.1217 - val_acc: 0.5881\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 7/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1141 - acc: 0.5834\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.59060\n",
            "49/49 [==============================] - 203s 4s/step - loss: 2.6653 - acc: 0.4703 - val_loss: 2.1141 - val_acc: 0.5834\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 8/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0913 - acc: 0.5880\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.59060\n",
            "49/49 [==============================] - 201s 4s/step - loss: 2.6611 - acc: 0.4736 - val_loss: 2.0913 - val_acc: 0.5880\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 9/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1118 - acc: 0.5865\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.59060\n",
            "49/49 [==============================] - 203s 4s/step - loss: 2.6637 - acc: 0.4731 - val_loss: 2.1118 - val_acc: 0.5865\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 10/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1055 - acc: 0.5852\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.59060\n",
            "49/49 [==============================] - 204s 4s/step - loss: 2.6445 - acc: 0.4774 - val_loss: 2.1055 - val_acc: 0.5852\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 11/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1083 - acc: 0.5860\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.59060\n",
            "49/49 [==============================] - 204s 4s/step - loss: 2.6501 - acc: 0.4738 - val_loss: 2.1083 - val_acc: 0.5860\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 12/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1015 - acc: 0.5859\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.59060\n",
            "49/49 [==============================] - 204s 4s/step - loss: 2.6443 - acc: 0.4758 - val_loss: 2.1015 - val_acc: 0.5859\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 13/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0846 - acc: 0.5905\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.59060\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "49/49 [==============================] - 202s 4s/step - loss: 2.6412 - acc: 0.4755 - val_loss: 2.0846 - val_acc: 0.5905\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 14/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0825 - acc: 0.5912\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.59060 to 0.59120, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinalv2:014-val_acc:0.591.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 9.999999747378752e-06\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 235s 5s/step - loss: 2.6260 - acc: 0.4807 - val_loss: 2.0825 - val_acc: 0.5912\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 15/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0891 - acc: 0.5931\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.59120 to 0.59310, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinalv2:015-val_acc:0.593.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 9.999999747378752e-06\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 210s 4s/step - loss: 2.6317 - acc: 0.4774 - val_loss: 2.0891 - val_acc: 0.5931\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 16/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0884 - acc: 0.5918\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.59310\n",
            "49/49 [==============================] - 183s 4s/step - loss: 2.6358 - acc: 0.4778 - val_loss: 2.0884 - val_acc: 0.5918\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 17/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0841 - acc: 0.5927\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.59310\n",
            "49/49 [==============================] - 205s 4s/step - loss: 2.6223 - acc: 0.4796 - val_loss: 2.0841 - val_acc: 0.5927\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 18/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0856 - acc: 0.5915\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.59310\n",
            "49/49 [==============================] - 202s 4s/step - loss: 2.6242 - acc: 0.4793 - val_loss: 2.0856 - val_acc: 0.5915\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 19/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0876 - acc: 0.5911\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.59310\n",
            "49/49 [==============================] - 196s 4s/step - loss: 2.6273 - acc: 0.4790 - val_loss: 2.0876 - val_acc: 0.5911\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 20/61\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0923 - acc: 0.5920\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.59310\n",
            "49/49 [==============================] - 195s 4s/step - loss: 2.6369 - acc: 0.4772 - val_loss: 2.0923 - val_acc: 0.5920\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 21/61\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0837 - acc: 0.5908\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.59310\n",
            "49/49 [==============================] - 195s 4s/step - loss: 2.6167 - acc: 0.4801 - val_loss: 2.0837 - val_acc: 0.5908\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 22/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0784 - acc: 0.5927\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.59310\n",
            "49/49 [==============================] - 195s 4s/step - loss: 2.6200 - acc: 0.4796 - val_loss: 2.0784 - val_acc: 0.5927\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 23/61\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0816 - acc: 0.5924\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.59310\n",
            "49/49 [==============================] - 193s 4s/step - loss: 2.6170 - acc: 0.4816 - val_loss: 2.0816 - val_acc: 0.5924\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 24/61\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0814 - acc: 0.5915\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.59310\n",
            "49/49 [==============================] - 194s 4s/step - loss: 2.6209 - acc: 0.4821 - val_loss: 2.0814 - val_acc: 0.5915\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 25/61\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0820 - acc: 0.5928\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.59310\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "49/49 [==============================] - 196s 4s/step - loss: 2.6096 - acc: 0.4817 - val_loss: 2.0820 - val_acc: 0.5928\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 26/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0793 - acc: 0.5924\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.59310\n",
            "49/49 [==============================] - 199s 4s/step - loss: 2.6140 - acc: 0.4803 - val_loss: 2.0793 - val_acc: 0.5924\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 27/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0865 - acc: 0.5905\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.59310\n",
            "49/49 [==============================] - 199s 4s/step - loss: 2.6079 - acc: 0.4839 - val_loss: 2.0865 - val_acc: 0.5905\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 28/61\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0825 - acc: 0.5901\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.59310\n",
            "49/49 [==============================] - 197s 4s/step - loss: 2.6171 - acc: 0.4793 - val_loss: 2.0825 - val_acc: 0.5901\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 29/61\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0815 - acc: 0.5907\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.59310\n",
            "49/49 [==============================] - 199s 4s/step - loss: 2.6217 - acc: 0.4780 - val_loss: 2.0815 - val_acc: 0.5907\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 30/61\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0826 - acc: 0.5910\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.59310\n",
            "49/49 [==============================] - 200s 4s/step - loss: 2.6157 - acc: 0.4798 - val_loss: 2.0826 - val_acc: 0.5910\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 31/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0791 - acc: 0.5901\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.59310\n",
            "49/49 [==============================] - 200s 4s/step - loss: 2.6091 - acc: 0.4816 - val_loss: 2.0791 - val_acc: 0.5901\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 32/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0818 - acc: 0.5907\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.59310\n",
            "49/49 [==============================] - 200s 4s/step - loss: 2.6140 - acc: 0.4804 - val_loss: 2.0818 - val_acc: 0.5907\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 33/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0838 - acc: 0.5913\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.59310\n",
            "49/49 [==============================] - 200s 4s/step - loss: 2.6113 - acc: 0.4837 - val_loss: 2.0838 - val_acc: 0.5913\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 34/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0748 - acc: 0.5913\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.59310\n",
            "49/49 [==============================] - 199s 4s/step - loss: 2.6096 - acc: 0.4812 - val_loss: 2.0748 - val_acc: 0.5913\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 35/61\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0808 - acc: 0.5909\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.59310\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "49/49 [==============================] - 200s 4s/step - loss: 2.6060 - acc: 0.4816 - val_loss: 2.0808 - val_acc: 0.5909\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 36/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0843 - acc: 0.5933\n",
            "\n",
            "Epoch 00036: val_acc improved from 0.59310 to 0.59330, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinalv2:036-val_acc:0.593.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 9.999999747378752e-06\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 227s 5s/step - loss: 2.6184 - acc: 0.4780 - val_loss: 2.0843 - val_acc: 0.5933\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 37/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0753 - acc: 0.5927\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.59330\n",
            "49/49 [==============================] - 174s 4s/step - loss: 2.6010 - acc: 0.4842 - val_loss: 2.0753 - val_acc: 0.5927\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 38/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0779 - acc: 0.5920\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.59330\n",
            "49/49 [==============================] - 201s 4s/step - loss: 2.6149 - acc: 0.4813 - val_loss: 2.0779 - val_acc: 0.5920\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 39/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0766 - acc: 0.5914\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.59330\n",
            "49/49 [==============================] - 199s 4s/step - loss: 2.5958 - acc: 0.4851 - val_loss: 2.0766 - val_acc: 0.5914\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 40/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0757 - acc: 0.5918\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.59330\n",
            "49/49 [==============================] - 200s 4s/step - loss: 2.6056 - acc: 0.4833 - val_loss: 2.0757 - val_acc: 0.5918\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 41/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0782 - acc: 0.5924\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.59330\n",
            "49/49 [==============================] - 200s 4s/step - loss: 2.6112 - acc: 0.4814 - val_loss: 2.0782 - val_acc: 0.5924\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 42/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0741 - acc: 0.5909\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.59330\n",
            "49/49 [==============================] - 200s 4s/step - loss: 2.6023 - acc: 0.4837 - val_loss: 2.0741 - val_acc: 0.5909\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 43/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0809 - acc: 0.5924\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.59330\n",
            "49/49 [==============================] - 200s 4s/step - loss: 2.6080 - acc: 0.4816 - val_loss: 2.0809 - val_acc: 0.5924\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 44/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0788 - acc: 0.5935\n",
            "\n",
            "Epoch 00044: val_acc improved from 0.59330 to 0.59350, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinalv2:044-val_acc:0.593.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 4.999999873689376e-06\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 229s 5s/step - loss: 2.5926 - acc: 0.4836 - val_loss: 2.0788 - val_acc: 0.5935\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 45/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0776 - acc: 0.5934\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.59350\n",
            "\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-07.\n",
            "49/49 [==============================] - 173s 4s/step - loss: 2.5970 - acc: 0.4844 - val_loss: 2.0776 - val_acc: 0.5934\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 46/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0761 - acc: 0.5930\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 201s 4s/step - loss: 2.6075 - acc: 0.4804 - val_loss: 2.0761 - val_acc: 0.5930\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 47/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0771 - acc: 0.5923\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 199s 4s/step - loss: 2.5950 - acc: 0.4841 - val_loss: 2.0771 - val_acc: 0.5923\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 48/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0760 - acc: 0.5923\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 199s 4s/step - loss: 2.5989 - acc: 0.4840 - val_loss: 2.0760 - val_acc: 0.5923\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 49/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0749 - acc: 0.5924\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 200s 4s/step - loss: 2.6006 - acc: 0.4842 - val_loss: 2.0749 - val_acc: 0.5924\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 50/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0772 - acc: 0.5920\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 199s 4s/step - loss: 2.5956 - acc: 0.4833 - val_loss: 2.0772 - val_acc: 0.5920\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 51/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0761 - acc: 0.5918\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 200s 4s/step - loss: 2.5926 - acc: 0.4842 - val_loss: 2.0761 - val_acc: 0.5918\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 52/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0783 - acc: 0.5922\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 199s 4s/step - loss: 2.5970 - acc: 0.4833 - val_loss: 2.0783 - val_acc: 0.5922\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 53/61\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0767 - acc: 0.5933\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 198s 4s/step - loss: 2.5952 - acc: 0.4833 - val_loss: 2.0767 - val_acc: 0.5933\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 54/61\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0764 - acc: 0.5921\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 198s 4s/step - loss: 2.6005 - acc: 0.4835 - val_loss: 2.0764 - val_acc: 0.5921\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 55/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0733 - acc: 0.5922\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.59350\n",
            "\n",
            "Epoch 00055: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-07.\n",
            "49/49 [==============================] - 199s 4s/step - loss: 2.5898 - acc: 0.4856 - val_loss: 2.0733 - val_acc: 0.5922\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 56/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0728 - acc: 0.5903\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 199s 4s/step - loss: 2.6037 - acc: 0.4823 - val_loss: 2.0728 - val_acc: 0.5903\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 57/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0752 - acc: 0.5910\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 198s 4s/step - loss: 2.5894 - acc: 0.4836 - val_loss: 2.0752 - val_acc: 0.5910\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 58/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0756 - acc: 0.5900\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 200s 4s/step - loss: 2.5890 - acc: 0.4849 - val_loss: 2.0756 - val_acc: 0.5900\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 59/61\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.0761 - acc: 0.5921\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 198s 4s/step - loss: 2.5898 - acc: 0.4850 - val_loss: 2.0761 - val_acc: 0.5921\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 60/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0753 - acc: 0.5923\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 199s 4s/step - loss: 2.5938 - acc: 0.4852 - val_loss: 2.0753 - val_acc: 0.5923\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 61/61\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.0743 - acc: 0.5916\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 200s 4s/step - loss: 2.5960 - acc: 0.4832 - val_loss: 2.0743 - val_acc: 0.5916\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f95489a9048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r83P_lFVNF_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    #Learning Rate Schedule\n",
        "    #\n",
        "    #Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    #Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "    #    epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "    #    lr (float32): learning rate\n",
        "    #\n",
        "    \n",
        "    if epoch > 100:\n",
        "        lr = 0.5e-5\n",
        "    elif epoch > 75:\n",
        "        lr = 1e-5\n",
        "    elif epoch > 50:\n",
        "        lr = 0.5e-4\n",
        "    elif epoch > 30:\n",
        "        lr = 1e-4\n",
        "    else:\n",
        "        lr = 1e-3\n",
        "    print('Learning rate (from LearningRateScheduler): ', lr)\n",
        "    return lr\n",
        "\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxC4MlqMzwkS",
        "colab_type": "code",
        "outputId": "b09e966c-c3b5-477e-9bd3-f6950e4d719d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11985
        }
      },
      "source": [
        "tpu_model.fit_generator(train_generator, \n",
        "                        epochs=105, \n",
        "                        steps_per_epoch=int(100000//batch_size),\n",
        "                        validation_steps=int(10000//batch_size), \n",
        "                        validation_data=validation_generator,\n",
        "                        callbacks=[checkpoint, lr_scheduler, csv_logger, lr_reducer] )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 1/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 3.2875 - acc: 0.4325\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 203s 4s/step - loss: 3.1485 - acc: 0.3657 - val_loss: 3.2875 - val_acc: 0.4325\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 2/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 3.4450 - acc: 0.4116\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 200s 4s/step - loss: 3.2488 - acc: 0.3544 - val_loss: 3.4450 - val_acc: 0.4116\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 3/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 3.3236 - acc: 0.4346\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 200s 4s/step - loss: 3.2450 - acc: 0.3609 - val_loss: 3.3236 - val_acc: 0.4346\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 4/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.9595 - acc: 0.4676\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 198s 4s/step - loss: 3.2340 - acc: 0.3624 - val_loss: 2.9595 - val_acc: 0.4676\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 5/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 3.0147 - acc: 0.4706\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 200s 4s/step - loss: 3.2320 - acc: 0.3670 - val_loss: 3.0147 - val_acc: 0.4706\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 6/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 3.3351 - acc: 0.4106\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 200s 4s/step - loss: 3.1763 - acc: 0.3782 - val_loss: 3.3351 - val_acc: 0.4106\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 7/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.7085 - acc: 0.5002\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 199s 4s/step - loss: 3.2568 - acc: 0.3632 - val_loss: 2.7085 - val_acc: 0.5002\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 8/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 4.0777 - acc: 0.3995\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 200s 4s/step - loss: 3.1895 - acc: 0.3773 - val_loss: 4.0777 - val_acc: 0.3995\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 9/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.7358 - acc: 0.5011\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 200s 4s/step - loss: 3.2228 - acc: 0.3727 - val_loss: 2.7358 - val_acc: 0.5011\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 10/105\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.9934 - acc: 0.4687\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 200s 4s/step - loss: 3.1834 - acc: 0.3796 - val_loss: 2.9934 - val_acc: 0.4687\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 11/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.9034 - acc: 0.4978\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 199s 4s/step - loss: 3.1767 - acc: 0.3815 - val_loss: 2.9034 - val_acc: 0.4978\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 12/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.8612 - acc: 0.4884\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 198s 4s/step - loss: 3.1793 - acc: 0.3820 - val_loss: 2.8612 - val_acc: 0.4884\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 13/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 3.1611 - acc: 0.4529\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 199s 4s/step - loss: 3.1584 - acc: 0.3848 - val_loss: 3.1611 - val_acc: 0.4529\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 14/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.8258 - acc: 0.4832\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 200s 4s/step - loss: 3.1566 - acc: 0.3876 - val_loss: 2.8258 - val_acc: 0.4832\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 15/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.7859 - acc: 0.4805\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 198s 4s/step - loss: 3.1698 - acc: 0.3846 - val_loss: 2.7859 - val_acc: 0.4805\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 16/105\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.9819 - acc: 0.4772\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 199s 4s/step - loss: 3.1611 - acc: 0.3877 - val_loss: 2.9819 - val_acc: 0.4772\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 17/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.9669 - acc: 0.4803\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.59350\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "49/49 [==============================] - 200s 4s/step - loss: 3.1492 - acc: 0.3885 - val_loss: 2.9669 - val_acc: 0.4803\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 18/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 3.9115 - acc: 0.3356\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 198s 4s/step - loss: 3.1355 - acc: 0.3921 - val_loss: 3.9115 - val_acc: 0.3356\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 19/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 3.1435 - acc: 0.4544\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 201s 4s/step - loss: 3.1693 - acc: 0.3846 - val_loss: 3.1435 - val_acc: 0.4544\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 20/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.8126 - acc: 0.4853\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 199s 4s/step - loss: 3.1668 - acc: 0.3875 - val_loss: 2.8126 - val_acc: 0.4853\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 21/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 3.2567 - acc: 0.4657\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 199s 4s/step - loss: 3.1396 - acc: 0.3922 - val_loss: 3.2567 - val_acc: 0.4657\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 22/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.8538 - acc: 0.4911\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 199s 4s/step - loss: 3.1428 - acc: 0.3902 - val_loss: 2.8538 - val_acc: 0.4911\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 23/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 3.0390 - acc: 0.4818\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 199s 4s/step - loss: 3.1307 - acc: 0.3952 - val_loss: 3.0390 - val_acc: 0.4818\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 24/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.7657 - acc: 0.4963\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 200s 4s/step - loss: 3.1430 - acc: 0.3925 - val_loss: 2.7657 - val_acc: 0.4963\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 25/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.9064 - acc: 0.4936\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 199s 4s/step - loss: 3.1406 - acc: 0.3929 - val_loss: 2.9064 - val_acc: 0.4936\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 26/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.8374 - acc: 0.4880\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 200s 4s/step - loss: 3.1198 - acc: 0.3967 - val_loss: 2.8374 - val_acc: 0.4880\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 27/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 3.3094 - acc: 0.4721\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.59350\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "49/49 [==============================] - 202s 4s/step - loss: 3.1239 - acc: 0.3958 - val_loss: 3.3094 - val_acc: 0.4721\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 28/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.9050 - acc: 0.4896\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 202s 4s/step - loss: 3.1205 - acc: 0.3976 - val_loss: 2.9050 - val_acc: 0.4896\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 29/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.9029 - acc: 0.4890\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 200s 4s/step - loss: 3.1272 - acc: 0.3961 - val_loss: 2.9029 - val_acc: 0.4890\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 30/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.8757 - acc: 0.4888\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 201s 4s/step - loss: 3.1141 - acc: 0.3998 - val_loss: 2.8757 - val_acc: 0.4888\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "Epoch 31/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.8890 - acc: 0.4846\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 200s 4s/step - loss: 3.1250 - acc: 0.3979 - val_loss: 2.8890 - val_acc: 0.4846\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 32/105\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.3112 - acc: 0.5799\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 199s 4s/step - loss: 2.9722 - acc: 0.4313 - val_loss: 2.3112 - val_acc: 0.5799\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 33/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.2888 - acc: 0.5829\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 200s 4s/step - loss: 2.8911 - acc: 0.4484 - val_loss: 2.2888 - val_acc: 0.5829\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 34/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.2674 - acc: 0.5839\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 200s 4s/step - loss: 2.8382 - acc: 0.4600 - val_loss: 2.2674 - val_acc: 0.5839\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 35/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.2478 - acc: 0.5872\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 199s 4s/step - loss: 2.8250 - acc: 0.4606 - val_loss: 2.2478 - val_acc: 0.5872\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 36/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.2274 - acc: 0.5877\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 199s 4s/step - loss: 2.8034 - acc: 0.4655 - val_loss: 2.2274 - val_acc: 0.5877\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 37/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.2286 - acc: 0.5859\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 199s 4s/step - loss: 2.7961 - acc: 0.4651 - val_loss: 2.2286 - val_acc: 0.5859\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 38/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.2568 - acc: 0.5890\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 199s 4s/step - loss: 2.7777 - acc: 0.4692 - val_loss: 2.2568 - val_acc: 0.5890\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 39/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.2158 - acc: 0.5878\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 198s 4s/step - loss: 2.7455 - acc: 0.4750 - val_loss: 2.2158 - val_acc: 0.5878\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 40/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.2314 - acc: 0.5895\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 199s 4s/step - loss: 2.7504 - acc: 0.4720 - val_loss: 2.2314 - val_acc: 0.5895\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 41/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.2370 - acc: 0.5888\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 200s 4s/step - loss: 2.7381 - acc: 0.4746 - val_loss: 2.2370 - val_acc: 0.5888\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 42/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.2109 - acc: 0.5857\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 200s 4s/step - loss: 2.7145 - acc: 0.4796 - val_loss: 2.2109 - val_acc: 0.5857\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 43/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1974 - acc: 0.5914\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 199s 4s/step - loss: 2.7176 - acc: 0.4764 - val_loss: 2.1974 - val_acc: 0.5914\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 44/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1859 - acc: 0.5922\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 201s 4s/step - loss: 2.6957 - acc: 0.4793 - val_loss: 2.1859 - val_acc: 0.5922\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 45/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1862 - acc: 0.5907\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 202s 4s/step - loss: 2.6758 - acc: 0.4840 - val_loss: 2.1862 - val_acc: 0.5907\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 46/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.2046 - acc: 0.5926\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 200s 4s/step - loss: 2.6866 - acc: 0.4815 - val_loss: 2.2046 - val_acc: 0.5926\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 47/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1968 - acc: 0.5926\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 202s 4s/step - loss: 2.6738 - acc: 0.4816 - val_loss: 2.1968 - val_acc: 0.5926\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 48/105\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.1797 - acc: 0.5918\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 201s 4s/step - loss: 2.6636 - acc: 0.4836 - val_loss: 2.1797 - val_acc: 0.5918\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 49/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1689 - acc: 0.5932\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 200s 4s/step - loss: 2.6466 - acc: 0.4861 - val_loss: 2.1689 - val_acc: 0.5932\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 50/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1926 - acc: 0.5884\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 201s 4s/step - loss: 2.6496 - acc: 0.4865 - val_loss: 2.1926 - val_acc: 0.5884\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "Epoch 51/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.2027 - acc: 0.5896\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.59350\n",
            "49/49 [==============================] - 200s 4s/step - loss: 2.6389 - acc: 0.4867 - val_loss: 2.2027 - val_acc: 0.5896\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 52/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1628 - acc: 0.5980\n",
            "\n",
            "Epoch 00052: val_acc improved from 0.59350 to 0.59800, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinalv2:052-val_acc:0.598.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 4.999999873689376e-05\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 229s 5s/step - loss: 2.6248 - acc: 0.4900 - val_loss: 2.1628 - val_acc: 0.5980\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 53/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1598 - acc: 0.5943\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.59800\n",
            "49/49 [==============================] - 174s 4s/step - loss: 2.6059 - acc: 0.4947 - val_loss: 2.1598 - val_acc: 0.5943\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 54/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1670 - acc: 0.5920\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.59800\n",
            "49/49 [==============================] - 199s 4s/step - loss: 2.6078 - acc: 0.4928 - val_loss: 2.1670 - val_acc: 0.5920\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 55/105\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.1600 - acc: 0.5940\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.59800\n",
            "49/49 [==============================] - 202s 4s/step - loss: 2.6036 - acc: 0.4936 - val_loss: 2.1600 - val_acc: 0.5940\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 56/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1569 - acc: 0.5968\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.59800\n",
            "49/49 [==============================] - 201s 4s/step - loss: 2.5912 - acc: 0.4957 - val_loss: 2.1569 - val_acc: 0.5968\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 57/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1696 - acc: 0.5941\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.59800\n",
            "49/49 [==============================] - 199s 4s/step - loss: 2.5946 - acc: 0.4929 - val_loss: 2.1696 - val_acc: 0.5941\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 58/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1575 - acc: 0.5970\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.59800\n",
            "49/49 [==============================] - 200s 4s/step - loss: 2.5970 - acc: 0.4944 - val_loss: 2.1575 - val_acc: 0.5970\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 59/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1422 - acc: 0.5971\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.59800\n",
            "49/49 [==============================] - 200s 4s/step - loss: 2.5801 - acc: 0.4963 - val_loss: 2.1422 - val_acc: 0.5971\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 60/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1567 - acc: 0.5958\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.59800\n",
            "49/49 [==============================] - 199s 4s/step - loss: 2.5872 - acc: 0.4957 - val_loss: 2.1567 - val_acc: 0.5958\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 61/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1539 - acc: 0.5946\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.59800\n",
            "49/49 [==============================] - 200s 4s/step - loss: 2.5803 - acc: 0.4958 - val_loss: 2.1539 - val_acc: 0.5946\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 62/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1469 - acc: 0.5989\n",
            "\n",
            "Epoch 00062: val_acc improved from 0.59800 to 0.59890, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinalv2:062-val_acc:0.599.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 4.999999873689376e-05\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "\n",
            "Epoch 00062: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
            "49/49 [==============================] - 228s 5s/step - loss: 2.5665 - acc: 0.4986 - val_loss: 2.1469 - val_acc: 0.5989\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 63/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1495 - acc: 0.5953\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.59890\n",
            "49/49 [==============================] - 172s 4s/step - loss: 2.5718 - acc: 0.4979 - val_loss: 2.1495 - val_acc: 0.5953\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 64/105\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.1510 - acc: 0.5964\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.59890\n",
            "49/49 [==============================] - 199s 4s/step - loss: 2.5628 - acc: 0.5003 - val_loss: 2.1510 - val_acc: 0.5964\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 65/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1520 - acc: 0.5967\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.59890\n",
            "49/49 [==============================] - 198s 4s/step - loss: 2.5602 - acc: 0.5007 - val_loss: 2.1520 - val_acc: 0.5967\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 66/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1356 - acc: 0.5958\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.59890\n",
            "49/49 [==============================] - 198s 4s/step - loss: 2.5573 - acc: 0.4992 - val_loss: 2.1356 - val_acc: 0.5958\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 67/105\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.1449 - acc: 0.5980\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.59890\n",
            "49/49 [==============================] - 199s 4s/step - loss: 2.5482 - acc: 0.5019 - val_loss: 2.1449 - val_acc: 0.5980\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 68/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1555 - acc: 0.5969\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.59890\n",
            "49/49 [==============================] - 198s 4s/step - loss: 2.5440 - acc: 0.5028 - val_loss: 2.1555 - val_acc: 0.5969\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 69/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1302 - acc: 0.5933\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.59890\n",
            "49/49 [==============================] - 198s 4s/step - loss: 2.5497 - acc: 0.5017 - val_loss: 2.1302 - val_acc: 0.5933\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 70/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1448 - acc: 0.5958\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.59890\n",
            "49/49 [==============================] - 199s 4s/step - loss: 2.5414 - acc: 0.5025 - val_loss: 2.1448 - val_acc: 0.5958\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 71/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1551 - acc: 0.5946\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.59890\n",
            "49/49 [==============================] - 199s 4s/step - loss: 2.5476 - acc: 0.5005 - val_loss: 2.1551 - val_acc: 0.5946\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 72/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1409 - acc: 0.5964\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.59890\n",
            "\n",
            "Epoch 00072: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
            "49/49 [==============================] - 201s 4s/step - loss: 2.5397 - acc: 0.5013 - val_loss: 2.1409 - val_acc: 0.5964\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 73/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1531 - acc: 0.5926\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.59890\n",
            "49/49 [==============================] - 201s 4s/step - loss: 2.5321 - acc: 0.5030 - val_loss: 2.1531 - val_acc: 0.5926\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 74/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1346 - acc: 0.5938\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.59890\n",
            "49/49 [==============================] - 200s 4s/step - loss: 2.5391 - acc: 0.5015 - val_loss: 2.1346 - val_acc: 0.5938\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 75/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1414 - acc: 0.5915\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.59890\n",
            "49/49 [==============================] - 201s 4s/step - loss: 2.5222 - acc: 0.5066 - val_loss: 2.1414 - val_acc: 0.5915\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "Epoch 76/105\n",
            "5/5 [==============================] - 14s 3s/step - loss: 2.1503 - acc: 0.5951\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.59890\n",
            "49/49 [==============================] - 202s 4s/step - loss: 2.5274 - acc: 0.5016 - val_loss: 2.1503 - val_acc: 0.5951\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 77/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1268 - acc: 0.5992\n",
            "\n",
            "Epoch 00077: val_acc improved from 0.59890 to 0.59920, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinalv2:077-val_acc:0.599.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 9.999999747378752e-06\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 231s 5s/step - loss: 2.5169 - acc: 0.5040 - val_loss: 2.1268 - val_acc: 0.5992\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 78/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1228 - acc: 0.5997\n",
            "\n",
            "Epoch 00078: val_acc improved from 0.59920 to 0.59970, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinalv2:078-val_acc:0.600.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 9.999999747378752e-06\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 205s 4s/step - loss: 2.5047 - acc: 0.5078 - val_loss: 2.1228 - val_acc: 0.5997\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 79/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1285 - acc: 0.6004\n",
            "\n",
            "Epoch 00079: val_acc improved from 0.59970 to 0.60040, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinalv2:079-val_acc:0.600.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 9.999999747378752e-06\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 203s 4s/step - loss: 2.5102 - acc: 0.5066 - val_loss: 2.1285 - val_acc: 0.6004\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 80/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1214 - acc: 0.6012\n",
            "\n",
            "Epoch 00080: val_acc improved from 0.60040 to 0.60120, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinalv2:080-val_acc:0.601.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 9.999999747378752e-06\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 205s 4s/step - loss: 2.4967 - acc: 0.5072 - val_loss: 2.1214 - val_acc: 0.6012\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 81/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1250 - acc: 0.6003\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.60120\n",
            "49/49 [==============================] - 176s 4s/step - loss: 2.5091 - acc: 0.5080 - val_loss: 2.1250 - val_acc: 0.6003\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 82/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1148 - acc: 0.5973\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.60120\n",
            "49/49 [==============================] - 203s 4s/step - loss: 2.4995 - acc: 0.5090 - val_loss: 2.1148 - val_acc: 0.5973\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 83/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1242 - acc: 0.5981\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.60120\n",
            "49/49 [==============================] - 202s 4s/step - loss: 2.5007 - acc: 0.5075 - val_loss: 2.1242 - val_acc: 0.5981\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 84/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1214 - acc: 0.5987\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.60120\n",
            "49/49 [==============================] - 203s 4s/step - loss: 2.4964 - acc: 0.5093 - val_loss: 2.1214 - val_acc: 0.5987\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 85/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1257 - acc: 0.5989\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 0.60120\n",
            "49/49 [==============================] - 201s 4s/step - loss: 2.5127 - acc: 0.5059 - val_loss: 2.1257 - val_acc: 0.5989\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 86/105\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.1266 - acc: 0.6006\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 0.60120\n",
            "49/49 [==============================] - 203s 4s/step - loss: 2.5060 - acc: 0.5069 - val_loss: 2.1266 - val_acc: 0.6006\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 87/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1233 - acc: 0.6017\n",
            "\n",
            "Epoch 00087: val_acc improved from 0.60120 to 0.60170, saving model to /content/gdrive/My Drive/epochs_TPUCustomImageAugFinalv2:087-val_acc:0.602.hdf5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 9.999999747378752e-06\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "49/49 [==============================] - 232s 5s/step - loss: 2.4967 - acc: 0.5094 - val_loss: 2.1233 - val_acc: 0.6017\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 88/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1222 - acc: 0.6005\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.60170\n",
            "49/49 [==============================] - 174s 4s/step - loss: 2.4924 - acc: 0.5104 - val_loss: 2.1222 - val_acc: 0.6005\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 89/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1232 - acc: 0.5997\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.60170\n",
            "49/49 [==============================] - 204s 4s/step - loss: 2.5051 - acc: 0.5070 - val_loss: 2.1232 - val_acc: 0.5997\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 90/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1219 - acc: 0.6010\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.60170\n",
            "49/49 [==============================] - 203s 4s/step - loss: 2.4958 - acc: 0.5093 - val_loss: 2.1219 - val_acc: 0.6010\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 91/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1184 - acc: 0.5993\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.60170\n",
            "49/49 [==============================] - 202s 4s/step - loss: 2.5048 - acc: 0.5070 - val_loss: 2.1184 - val_acc: 0.5993\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 92/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1185 - acc: 0.5999\n",
            "\n",
            "Epoch 00092: val_acc did not improve from 0.60170\n",
            "49/49 [==============================] - 203s 4s/step - loss: 2.5046 - acc: 0.5077 - val_loss: 2.1185 - val_acc: 0.5999\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 93/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1160 - acc: 0.6013\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 0.60170\n",
            "49/49 [==============================] - 202s 4s/step - loss: 2.5007 - acc: 0.5078 - val_loss: 2.1160 - val_acc: 0.6013\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 94/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1168 - acc: 0.6012\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.60170\n",
            "49/49 [==============================] - 202s 4s/step - loss: 2.4919 - acc: 0.5094 - val_loss: 2.1168 - val_acc: 0.6012\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 95/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1147 - acc: 0.6012\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 0.60170\n",
            "49/49 [==============================] - 203s 4s/step - loss: 2.4814 - acc: 0.5112 - val_loss: 2.1147 - val_acc: 0.6012\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 96/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1225 - acc: 0.5996\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.60170\n",
            "49/49 [==============================] - 202s 4s/step - loss: 2.4983 - acc: 0.5075 - val_loss: 2.1225 - val_acc: 0.5996\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 97/105\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.1115 - acc: 0.5995\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.60170\n",
            "\n",
            "Epoch 00097: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "49/49 [==============================] - 201s 4s/step - loss: 2.4790 - acc: 0.5116 - val_loss: 2.1115 - val_acc: 0.5995\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 98/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1157 - acc: 0.5994\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.60170\n",
            "49/49 [==============================] - 201s 4s/step - loss: 2.4844 - acc: 0.5112 - val_loss: 2.1157 - val_acc: 0.5994\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 99/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1179 - acc: 0.5985\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.60170\n",
            "49/49 [==============================] - 201s 4s/step - loss: 2.4988 - acc: 0.5071 - val_loss: 2.1179 - val_acc: 0.5985\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 100/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1138 - acc: 0.5982\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.60170\n",
            "49/49 [==============================] - 201s 4s/step - loss: 2.4798 - acc: 0.5121 - val_loss: 2.1138 - val_acc: 0.5982\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "Epoch 101/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1132 - acc: 0.5999\n",
            "\n",
            "Epoch 00101: val_acc did not improve from 0.60170\n",
            "49/49 [==============================] - 203s 4s/step - loss: 2.4931 - acc: 0.5096 - val_loss: 2.1132 - val_acc: 0.5999\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 102/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1107 - acc: 0.6009\n",
            "\n",
            "Epoch 00102: val_acc did not improve from 0.60170\n",
            "49/49 [==============================] - 201s 4s/step - loss: 2.4907 - acc: 0.5095 - val_loss: 2.1107 - val_acc: 0.6009\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 103/105\n",
            "5/5 [==============================] - 12s 2s/step - loss: 2.1105 - acc: 0.5999\n",
            "\n",
            "Epoch 00103: val_acc did not improve from 0.60170\n",
            "49/49 [==============================] - 201s 4s/step - loss: 2.4794 - acc: 0.5130 - val_loss: 2.1105 - val_acc: 0.5999\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 104/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1108 - acc: 0.5994\n",
            "\n",
            "Epoch 00104: val_acc did not improve from 0.60170\n",
            "49/49 [==============================] - 201s 4s/step - loss: 2.4933 - acc: 0.5092 - val_loss: 2.1108 - val_acc: 0.5994\n",
            "Learning rate (from LearningRateScheduler):  5e-06\n",
            "Epoch 105/105\n",
            "5/5 [==============================] - 13s 3s/step - loss: 2.1096 - acc: 0.6004\n",
            "\n",
            "Epoch 00105: val_acc did not improve from 0.60170\n",
            "49/49 [==============================] - 200s 4s/step - loss: 2.4754 - acc: 0.5136 - val_loss: 2.1096 - val_acc: 0.6004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f954821ba20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    }
  ]
}